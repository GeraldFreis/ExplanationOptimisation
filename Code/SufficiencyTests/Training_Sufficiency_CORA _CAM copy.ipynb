{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains a model with additional loss for Sufficiency on CORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains a model with additional loss for Sufficiency on CORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import numpy as np\n",
    "import torch\n",
    "dataset = Planetoid(root='/tmp/Cora/', name='Cora')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\freis\\anaconda3\\envs\\mainEnv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import k_hop_subgraph, to_undirected\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.data import Data\n",
    "from graphxai.explainers._base import _BaseExplainer\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "\n",
    "class _BaseExplainer:\n",
    "    \"\"\"\n",
    "    Base Class for Explainers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            model: nn.Module,\n",
    "            emb_layer_name: Optional[str] = None,\n",
    "            is_subgraphx: Optional[bool] = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "                The output of the model should be unnormalized class score.\n",
    "                For example, last layer = GCNConv or Linear.\n",
    "            emb_layer_name (str, optional): name of the embedding layer\n",
    "                If not specified, use the last but one layer by default.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.L = len([module for module in self.model.modules()\n",
    "                      if isinstance(module, MessagePassing)])\n",
    "        self.explain_graph = False  # Assume node-level explanation by default\n",
    "        self.subgraphx_flag = is_subgraphx\n",
    "        self.__set_embedding_layer(emb_layer_name)\n",
    "\n",
    "    def __set_embedding_layer(self, emb_layer_name: str = None):\n",
    "        \"\"\"\n",
    "        Set the embedding layer (by default is the last but one layer).\n",
    "        \"\"\"\n",
    "        if emb_layer_name:\n",
    "            try:\n",
    "                self.emb_layer = getattr(self.model, emb_layer_name)\n",
    "            except AttributeError:\n",
    "                raise ValueError(f'{emb_layer_name} does not exist in the model')\n",
    "        else:\n",
    "            self.emb_layer = list(self.model.modules())[-2]\n",
    "\n",
    "    def _get_embedding(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                       forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the embedding.\n",
    "        \"\"\"\n",
    "        emb = self._get_activation(self.emb_layer, x, edge_index, forward_kwargs)\n",
    "        return emb\n",
    "\n",
    "    def _set_masks(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                   edge_mask: torch.Tensor = None, explain_feature: bool = False,\n",
    "                   device = None):\n",
    "        \"\"\"\n",
    "        Initialize the edge (and feature) masks.\n",
    "        \"\"\"\n",
    "        (n, d), m = x.shape, edge_index.shape[1]\n",
    "\n",
    "        # Initialize edge_mask and feature_mask for learning\n",
    "        std = torch.nn.init.calculate_gain('relu') * np.sqrt(2.0 / (2 * n))\n",
    "        if edge_mask is None:\n",
    "            edge_mask = (torch.randn(m) * std).to(device)\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        else:\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        if explain_feature:\n",
    "            feature_mask = (torch.randn(d) * 0.1).to(device)\n",
    "            self.feature_mask = torch.nn.Parameter(feature_mask)\n",
    "\n",
    "        self.loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "        # Tell pytorch geometric to apply edge masks\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "                module.__loop_mask__ = self.loop_mask\n",
    "\n",
    "    def _clear_masks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.edge_mask = None\n",
    "        self.feature_mask = None\n",
    "\n",
    "    def _flow(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def _predict(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                 return_type: str = 'label', forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the model's prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            return_type (str): one of ['label', 'prob', 'log_prob']\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            pred (torch.Tensor, [n x ...]): model prediction\n",
    "        \"\"\"\n",
    "        # Compute unnormalized class score\n",
    "        with torch.no_grad():\n",
    "            out = self.model.to(device)(x, edge_index, **forward_kwargs)\n",
    "            if return_type == 'label':\n",
    "                out = out.argmax(dim=-1)\n",
    "            elif return_type == 'prob':\n",
    "                out = F.softmax(out, dim=-1)\n",
    "            elif return_type == 'log_prob':\n",
    "                out = F.log_softmax(out, dim=-1)\n",
    "            else:\n",
    "                raise ValueError(\"return_type must be 'label', 'prob', or 'log_prob'\")\n",
    "\n",
    "            if self.explain_graph:\n",
    "                out = out.squeeze()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _prob_score_func_graph(self, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probability that the input graphs\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            target_class (int): the targeted class of the graph\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        def get_prob_score(x: torch.Tensor,\n",
    "                           edge_index: torch.Tensor,\n",
    "                           forward_kwargs: dict = {}):\n",
    "            prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                 forward_kwargs=forward_kwargs)\n",
    "            score = prob[:, target_class]\n",
    "            return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _prob_score_func_node(self, node_idx: torch.Tensor, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probabilities that k specified nodes\n",
    "        in `torch_geometric.data.Batch` (disconnected union of the input graphs)\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            node_idx (torch.Tensor, [k]): the indices of the k nodes interested\n",
    "            target_class (torch.Tensor, [k]): the targeted classes of the k nodes\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        if self.subgraphx_flag:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[node_idx, target_class]\n",
    "                return score\n",
    "        else:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[:, node_idx, target_class]\n",
    "                return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _get_activation(self, layer: nn.Module, x: torch.Tensor,\n",
    "                        edge_index: torch.Tensor, forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the activation of the layer.\n",
    "        \"\"\"\n",
    "        activation = {}\n",
    "        def get_activation():\n",
    "            def hook(model, inp, out):\n",
    "                activation['layer'] = out.detach()\n",
    "            return hook\n",
    "\n",
    "        layer.register_forward_hook(get_activation())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return activation['layer']\n",
    "\n",
    "    def _get_k_hop_subgraph(self, node_idx: int, x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor, num_hops: int = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Extract the subgraph of target node\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): the node index\n",
    "            x (torch.Tensor, [n x d]): node feature matrix with shape\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index\n",
    "            kwargs (dict): additional parameters of the graph\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # TODO: use NamedTuple\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        return khop_info\n",
    "\n",
    "    def get_explanation_node(self, node_idx: int,\n",
    "                             x: torch.Tensor,\n",
    "                             edge_index: torch.Tensor,\n",
    "                             label: torch.Tensor = None,\n",
    "                             num_hops: int = None,\n",
    "                             forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved\n",
    "        \"\"\"\n",
    "        # If labels are needed\n",
    "        label = self._predict(x, edge_index, return_type='label') if label is None else label\n",
    "        # If probabilities / log probabilities are needed\n",
    "        prob = self._predict(x, edge_index, return_type='prob')\n",
    "        log_prob = self._predict(x, edge_index, return_type='log_prob')\n",
    "\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return exp, khop_info\n",
    "\n",
    "    def get_explanation_graph(self, edge_index: torch.Tensor,\n",
    "                              x: torch.Tensor, label: torch.Tensor,\n",
    "                              forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a whole-graph prediction.\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            label (torch.Tensor, [n x ...]): labels to explain\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "        \"\"\"\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "from graphxai.utils import Explanation\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph as subgraph\n",
    "\n",
    "class GNNPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class _BaseDecomposition(_BaseExplainer):\n",
    "    '''\n",
    "    Code adapted from Dive into Graphs (DIG)\n",
    "    Code: https://github.com/divelab/DIG\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__(model=model) # Will set self.model = model\n",
    "        # Other properties: self.L (number of layers)\n",
    "\n",
    "    @property\n",
    "    def __num_hops__(self):\n",
    "        if self.explain_graph:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.L\n",
    "    \n",
    "    def set_graph_attr(self,\n",
    "                x: Tensor,\n",
    "                edge_index: Tensor,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.num_edges = edge_index.shape[1]\n",
    "        self.num_nodes = x.shape[0]\n",
    "        self.device = x.device\n",
    "\n",
    "    def extract_step(self, x: Tensor, edge_index: Tensor, detach: bool = True, split_fc: bool = False, forward_kwargs: dict = None):\n",
    "        '''Gets information about every layer in the graph\n",
    "        Args:\n",
    "\n",
    "            forward_kwargs (tuple, optional): Additional arguments to model forward call (other than x and edge_index)\n",
    "                (default: :obj:`None`)\n",
    "        '''\n",
    "\n",
    "        layer_extractor = []\n",
    "        hooks = []\n",
    "\n",
    "        def register_hook(module: nn.Module):\n",
    "            if not list(module.children()) or isinstance(module, MessagePassing):\n",
    "                hooks.append(module.register_forward_hook(forward_hook))\n",
    "\n",
    "        def forward_hook(module: nn.Module, input: Tuple[Tensor], output: Tensor):\n",
    "            # input contains x and edge_index\n",
    "            if detach:\n",
    "                layer_extractor.append((module, input[0].clone().detach(), output.clone().detach()))\n",
    "            else:\n",
    "                layer_extractor.append((module, input[0], output))\n",
    "\n",
    "        # --- register hooks ---\n",
    "        self.model.apply(register_hook)\n",
    "\n",
    "        # ADDED: OWEN QUEEN --------------\n",
    "        if forward_kwargs is None:\n",
    "            _ = self.model(x, edge_index)\n",
    "        else:\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "        # --------------------------------\n",
    "        # Remove hooks:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # --- divide layer sets ---\n",
    "\n",
    "        # print('Layer extractor', [layer_extractor[i][0] for i in range(len(layer_extractor))])\n",
    "\n",
    "        walk_steps = []\n",
    "        fc_steps = []\n",
    "        pool_flag = False\n",
    "        step = {'input': None, 'module': [], 'output': None}\n",
    "        for layer in layer_extractor:\n",
    "            if isinstance(layer[0], MessagePassing):\n",
    "                if step['module']: # Append step that had previously been building\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], GNNPool):\n",
    "                pool_flag = True\n",
    "                if step['module']:\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                # Putting in GNNPool\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], nn.Linear):\n",
    "                if step['module']:\n",
    "                    if isinstance(step['module'][0], MessagePassing):\n",
    "                        walk_steps.append(step) # Append MessagePassing layer to walk_steps\n",
    "                    else: # Always append Linear layers to fc_steps\n",
    "                        fc_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            # Also appends non-trainable layers to step (not modifying input):\n",
    "            step['module'].append(layer[0])\n",
    "            step['output'] = layer[2]\n",
    "\n",
    "        if step['module']:\n",
    "            if isinstance(step['module'][0], MessagePassing):\n",
    "                walk_steps.append(step)\n",
    "            else: # Append anything to FC that is not MessagePassing at its origin\n",
    "                # Still supports sequential layers\n",
    "                fc_steps.append(step)\n",
    "            # print('layer', layer[0])\n",
    "            # if isinstance(layer[0], MessagePassing) or isinstance(layer[0], GNNPool):\n",
    "            #     if isinstance(layer[0], GNNPool):\n",
    "            #         pool_flag = True\n",
    "            #     if step['module'] and step['input'] is not None:\n",
    "            #         walk_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # if pool_flag and split_fc and isinstance(layer[0], nn.Linear):\n",
    "            #     if step['module']:\n",
    "            #         fc_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # step['module'].append(layer[0])\n",
    "            # step['output'] = layer[2]\n",
    "\n",
    "        for walk_step in walk_steps:\n",
    "            if hasattr(walk_step['module'][0], 'nn') and walk_step['module'][0].nn is not None:\n",
    "                # We don't allow any outside nn during message flow process in GINs\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "            elif hasattr(walk_step['module'][0], 'lin') and walk_step['module'][0].lin is not None:\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "\n",
    "        # print('Walk steps', [walk_steps[i]['module'] for i in range(len(walk_steps))])\n",
    "        # print('fc steps', [fc_steps[i]['module'] for i in range(len(fc_steps))])\n",
    "\n",
    "        return walk_steps, fc_steps\n",
    "\n",
    "    def walks_pick(self,\n",
    "                   edge_index: Tensor,\n",
    "                   pick_edge_indices: List,\n",
    "                   walk_indices: List=[],\n",
    "                   num_layers=0\n",
    "                   ):\n",
    "        walk_indices_list = []\n",
    "        for edge_idx in pick_edge_indices:\n",
    "\n",
    "            # Adding one edge\n",
    "            walk_indices.append(edge_idx)\n",
    "            _, new_src = src, tgt = edge_index[:, edge_idx]\n",
    "            next_edge_indices = np.array((edge_index[0, :] == new_src).nonzero().view(-1))\n",
    "\n",
    "            # Finding next edge\n",
    "            if len(walk_indices) >= num_layers:\n",
    "                # return one walk\n",
    "                walk_indices_list.append(walk_indices.copy())\n",
    "            else:\n",
    "                walk_indices_list += self.walks_pick(edge_index, next_edge_indices, walk_indices, num_layers)\n",
    "\n",
    "            # remove the last edge\n",
    "            walk_indices.pop(-1)\n",
    "\n",
    "        return walk_indices_list\n",
    "\n",
    "class CAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, activation = None):\n",
    "        '''\n",
    "        .. note::\n",
    "            From Pope et al., CAM requires that the layer immediately before the softmax layer be\n",
    "            a global average pooling layer, or in the case of node classification, a graph convolutional\n",
    "            layer. Therefore, for this algorithm to theoretically work, there can be no fully-connected\n",
    "            layers after global pooling. There is no restriction in the code for this, but be warned. \n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            activation (method, optional): activation funciton for final layer in network. If `activation = None`,\n",
    "                explainer assumes linear activation. Use `activation = None` if the activation is applied\n",
    "                within the `forward` method of `model`, only set this parameter if another activation is\n",
    "                applied in the training procedure outside of model. (:default: :obj:`None`)\n",
    "        '''\n",
    "        super().__init__(model=model)\n",
    "        self.model = model\n",
    "\n",
    "        # Set activation function\n",
    "        self.activation = lambda x: x  if activation is None else activation\n",
    "        # i.e. linear activation if none provided\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "                x: torch.Tensor, \n",
    "                node_idx: int, \n",
    "                edge_index: torch.Tensor, \n",
    "                label: int = None,  \n",
    "                y = None,\n",
    "                forward_kwargs: dict = {},\n",
    "                directed: bool = False\n",
    "            ) -> Explanation:\n",
    "        '''\n",
    "        Explain one node prediction by the model\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of node features from the entire graph\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.tensor): edge_index of entire graph\n",
    "            label (int, optional): Label on which to compute the explanation for\n",
    "                this node. If `None`, the predicted label from the model will be\n",
    "                used. (default: :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "            directed (bool, optional): If True, graph is directed.\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        if not directed:\n",
    "            edge_index = to_undirected(edge_index)\n",
    "\n",
    "        if label is None:\n",
    "            if y is None:\n",
    "                label = int(self.__forward_pass(x, edge_index, forward_kwargs).argmax(dim=1).item())\n",
    "            else:\n",
    "                label = y[node_idx]\n",
    "\n",
    "        # Perform walk:\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=False, split_fc=False, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        # Get subgraph:\n",
    "        khop_info = k_hop_subgraph(node_idx = node_idx, num_hops = self.L, edge_index = edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        #cam = torch.zeros(N) # Compute CAM only over the subgraph (all others are zero)\n",
    "        cam = torch.zeros(subgraph_N)\n",
    "        for i in range(subgraph_N):\n",
    "            n = subgraph_nodes[i]\n",
    "            cam[i] += self.__exp_node(n, walk_steps, label)\n",
    "\n",
    "        # Set Explanation class:\n",
    "        exp = Explanation(\n",
    "            node_imp = cam,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def __forward_pass(self, x, edge_index, forward_kwargs = {}):\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, predicted_c):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after last convolutiuonal layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        last_conv_layer = walk_steps[-1]\n",
    "\n",
    "        if isinstance(last_conv_layer['module'][0], GINConv):\n",
    "            weight_vec = last_conv_layer['module'][0].nn.weight[predicted_c, :].detach()  # last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], GCNConv):\n",
    "            weight_vec = last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], torch.nn.Linear):\n",
    "            weight_vec = last_conv_layer['module'][0].weight[predicted_c, :].detach()\n",
    "\n",
    "        F_l_n = F.relu(last_conv_layer['input'][node_idx,:]).detach()\n",
    "\n",
    "        L_cam_n = F.relu(torch.matmul(weight_vec, F_l_n))\n",
    "\n",
    "        return L_cam_n.item()\n",
    "\n",
    "\n",
    "class GradCAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Gradient Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.Tensor, criterion = F.cross_entropy):\n",
    "        '''\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            criterion (PyTorch Loss Function): loss function used to train the model.\n",
    "                Needed to pass gradients backwards in the network to obtain gradients.\n",
    "        '''\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "            x: torch.Tensor, \n",
    "            y: torch.Tensor, \n",
    "            node_idx: int, \n",
    "            edge_index: torch.Tensor, \n",
    "            label: int = None, \n",
    "            forward_kwargs: dict = {}, \n",
    "            average_variant: bool = True, \n",
    "            layer: int = 0\n",
    "        ) -> Explanation:\n",
    "        '''\n",
    "        Explain a node in the given graph\n",
    "        Args:\n",
    "            x (torch.Tensor, (n,)): Tensor of node features from the entire graph, with n nodes.\n",
    "            y (torch.Tensor, (n,)): Ground-truth labels for all n nodes in the graph.\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.Tensor): Edge_index of entire graph.\n",
    "            label (int, optional): Label for which to compute Grad-CAM against. If None, computes\n",
    "                the Grad-CAM with respect to the model's predicted class for this node.\n",
    "                (default :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. (default: :obj:`None`)\n",
    "            average_variant (bool, optional): If True, computes the average Grad-CAM across all convolutional\n",
    "                layers in the model. If False, computes Grad-CAM for `layer`. (default: :obj:`True`)\n",
    "            layer (int, optional): Layer by which to compute the Grad-CAM. Argument only has an effect if \n",
    "                `average_variant == True`. Must be less-than the total number of convolutional layers\n",
    "                in the model. (default: :obj:`0`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        x = x.detach().clone()\n",
    "        y = y.detach().clone()\n",
    "\n",
    "        x.requires_grad = True\n",
    "\n",
    "        if label is None:\n",
    "            pred = self.__forward_pass(x, y, edge_index, forward_kwargs)[0][node_idx, :].reshape(1, -1)\n",
    "            y[node_idx] = pred.argmax(dim=1).item()\n",
    "        else: # Transform node_idx's label if provided by user\n",
    "            pred, loss = self.__forward_pass(x, y, edge_index, forward_kwargs)\n",
    "            y[node_idx] = label\n",
    "\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=True, split_fc=True, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        khop_info = k_hop_subgraph(node_idx, self.L, edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        if average_variant:\n",
    "            # Size of all nodes in the subgraph:\n",
    "            avg_gcam = torch.zeros(subgraph_N)\n",
    "\n",
    "            for l in range(self.L):\n",
    "                # Compute gradients for this layer ahead of time:\n",
    "                gradients = self.__grad_by_layer(l)\n",
    "\n",
    "                for i in range(subgraph_N): # Over all subgraph nodes\n",
    "                    n = subgraph_nodes[i]\n",
    "                    avg_gcam[i] += self.__get_gCAM_layer(walk_steps, l, n, gradients)\n",
    "\n",
    "            avg_gcam /= self.L # Apply average\n",
    "\n",
    "            exp.node_imp = avg_gcam\n",
    "\n",
    "        else:\n",
    "            assert layer < len(walk_steps), \"Layer must be an index of convolutional layers\"\n",
    "\n",
    "            gcam = torch.zeros(subgraph_N)\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "            for i in range(subgraph_N):\n",
    "                n = subgraph_nodes[i]\n",
    "                gcam[i] += self.__get_gCAM_layer(walk_steps, layer, n, gradients)#[0]\n",
    "\n",
    "            exp.node_imp = gcam\n",
    "\n",
    "        return exp\n",
    "        \n",
    "    def __forward_pass(self, x, label, edge_index, forward_kwargs):\n",
    "        x.requires_grad = True # Enforce that x needs gradient\n",
    "\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        loss = self.criterion(pred, label)\n",
    "        loss.backward() # Propagate loss backward through network\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def __grad_by_layer(self, layer):\n",
    "        # Index 0 of parameters to avoid calculating gradients for biases\n",
    "        module_at_layer = list(self.model.children())[layer]\n",
    "\n",
    "        if isinstance(module_at_layer, GCNConv):\n",
    "            grad = module_at_layer.lin.weight.grad\n",
    "        elif isinstance(module_at_layer, GINConv):\n",
    "            grad = module_at_layer.nn.weight.grad\n",
    "        else:\n",
    "            grad = module_at_layer.weight.grad\n",
    "\n",
    "        return grad.mean(dim=1)\n",
    "\n",
    "    def __get_gCAM_layer(self, walk_steps, layer, node_idx = None, gradients = None):\n",
    "        # Gets Grad CAM for one layer\n",
    "        if gradients is None:\n",
    "            # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "            # ''        '' shape: [k,] - k= # output features from layer l\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "\n",
    "        if node_idx is None: # Need to compute for entire graph:\n",
    "            node_explanations = []\n",
    "            for n in range(self.N):\n",
    "                node_explanations.append(self.__exp_node(n, walk_steps, layer, gradients))\n",
    "\n",
    "            return node_explanations\n",
    "\n",
    "        # Return for only one node:\n",
    "        return self.__exp_node(node_idx, walk_steps, layer, gradients)\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, layer, gradients):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after each convolutional layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "        # ''        '' shape: [k,] - k= # input features to layer l\n",
    "\n",
    "        # Activations for node n\n",
    "        F_l_n = F.relu(walk_steps[layer]['output'][node_idx,:]).detach()\n",
    "        L_cam_n = F.relu(torch.matmul(gradients, F_l_n)) # Combine gradients and activations\n",
    "\n",
    "        return L_cam_n.item()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from graphxai.utils import Explanation\n",
    "# from ._decomp_base_old import _BaseDecomposition\n",
    "\n",
    "def clip_hook(grad):\n",
    "    # Apply ReLU activation to gradient\n",
    "    return torch.clamp(grad, min=0)\n",
    "import gc\n",
    "\n",
    "class GuidedBP(_BaseDecomposition):\n",
    "\n",
    "    def __init__(self, model, criterion = F.cross_entropy, enforce_requires_grad = True):\n",
    "        '''\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            criterion (PyTorch Loss Function): loss function used to train the model.\n",
    "                Needed to pass gradients backwards in the network to obtain gradients.\n",
    "        '''\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.L = len([module for module in self.model.modules() if isinstance(module, MessagePassing)])\n",
    "\n",
    "        self.registered_hooks = []\n",
    "\n",
    "        self.enforce_requires_grad = enforce_requires_grad\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "                x: torch.Tensor, \n",
    "                y: torch.Tensor,\n",
    "                edge_index: torch.Tensor,  \n",
    "                node_idx: int, \n",
    "                aggregate_node_imp = torch.sum,\n",
    "                forward_kwargs: dict = {}\n",
    "            ) -> Explanation:\n",
    "        '''\n",
    "        Get Guided Backpropagation explanation for one node in the graph\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of node features from the entire graph\n",
    "            y (torch.Tensor): Ground truth labels correspond to each node's \n",
    "                classification. This argument is input to the `criterion` \n",
    "                function provided in `__init__()`.\n",
    "            edge_index (torch.tensor): Edge_index of entire graph.\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the enclosing \n",
    "                subgraph. Must support `dim` argument.\n",
    "                (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        # Run whole-graph prediction:\n",
    "        if self.enforce_requires_grad:\n",
    "            try:\n",
    "                x = x.detach().clone()\n",
    "                x.requires_grad = True\n",
    "            except:\n",
    "                pass\n",
    "        # assert x.requires_grad, 'x must have requires_grad == True'\n",
    "\n",
    "        # Perform the guided backprop:\n",
    "        xhook = x.register_hook(clip_hook)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        pred = self.__forward_pass(x, edge_index, forward_kwargs)\n",
    "        loss = self.criterion(pred, y)\n",
    "        self.__apply_hooks()\n",
    "        loss.backward()\n",
    "        self.__rm_hooks()\n",
    "\n",
    "        xhook.remove() # Remove hook from x\n",
    "\n",
    "        graph_exp = x.grad\n",
    "\n",
    "        khop_info = k_hop_subgraph(node_idx = node_idx, num_hops = self.L, edge_index = edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        node_imp = aggregate_node_imp(torch.stack([graph_exp[i,:] for i in subgraph_nodes]).detach(), dim=1)\n",
    "\n",
    "        # Get only those explanations for nodes in the subgraph:\n",
    "        exp = Explanation(\n",
    "            node_imp = node_imp,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        \n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_graph(self, \n",
    "                x: torch.Tensor, \n",
    "                y: torch.Tensor, \n",
    "                edge_index: torch.Tensor, \n",
    "                aggregate_node_imp = torch.sum,\n",
    "                forward_kwargs: dict = {}\n",
    "        ) -> Explanation:\n",
    "        '''\n",
    "        Explain a whole-graph prediction with Guided Backpropagation\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Tensor of node features from the entire graph.\n",
    "            y (torch.tensor): Ground truth label of given input. This argument is \n",
    "                input to the `criterion` function provided in `__init__()`.\n",
    "            edge_index (torch.tensor): Edge_index of entire graph.\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the graph. \n",
    "                Must support `dim` argument. (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)   \n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method. \n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [num_nodes, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `graph`: :obj:`torch_geometric.data.Data`\n",
    "        '''\n",
    "\n",
    "        # Run whole-graph prediction:\n",
    "        try:\n",
    "            x.requires_grad = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        assert x.requires_grad, 'x must have requires_grad == True' \n",
    "\n",
    "        # Perform the guided backprop:\n",
    "        xhook = x.register_hook(clip_hook)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        pred = self.__forward_pass(x, edge_index, forward_kwargs)\n",
    "        loss = self.criterion(pred, y)\n",
    "        self.__apply_hooks()\n",
    "        loss.backward()\n",
    "        self.__rm_hooks()\n",
    "\n",
    "        xhook.remove() # Remove hook from x\n",
    "\n",
    "        node_imp = aggregate_node_imp(x.grad, dim=1)\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_imp = node_imp\n",
    "        )\n",
    "    \n",
    "        exp.set_whole_graph(Data(x, edge_index))\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def __apply_hooks(self):\n",
    "        self.registered_hooks = []\n",
    "        for p in self.model.parameters():\n",
    "            h = p.register_hook(clip_hook)\n",
    "            self.registered_hooks.append(h)\n",
    "\n",
    "    def __rm_hooks(self):\n",
    "        for h in self.registered_hooks:\n",
    "            h.remove()\n",
    "        self.registered_hooks = []\n",
    "    \n",
    "    def __forward_pass(self, x, edge_index, forward_kwargs):\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        self.__apply_hooks()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return pred\n",
    "from graphxai.utils import Explanation\n",
    "class IG():\n",
    "    def __init__(self, model, hops: Optional[int] = 1, criterion = None):\n",
    "        self.model = model\n",
    "        self.num_hops = hops\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def node_explanations(self, node_idx: int, \n",
    "            x: torch.Tensor,\n",
    "            edge_index: torch.Tensor, \n",
    "            y: Optional[torch.Tensor] = None,):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): Index of the node to be explained.\n",
    "            edge_index (torch.Tensor, [2 x m]): Edge index of the graph.\n",
    "            x (torch.Tensor, [n x d]): Node features.\n",
    "            label (torch.Tensor, [n x ...]): Labels to explain.\n",
    "            y (torch.Tensor): Same as `label`, provided for general \n",
    "                compatibility in the arguments. (:default: :obj:`None`)\n",
    "            num_hops (int, optional): Number of hops in the enclosing \n",
    "                subgraph. If `None`, set to the number of layers in \n",
    "                the GNN. (:default: :obj:`None`)\n",
    "            steps (int, optional): Number of steps for the Riemannian \n",
    "                integration. (:default: :obj:`40`)\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`torch.Tensor, [x.shape[1],]`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :class:`graphxai.utils.EnclosingSubgraph`\n",
    "        \"\"\"\n",
    "\n",
    "        if (y is None):\n",
    "            raise ValueError('Either label or y should be provided for Integrated Gradients')\n",
    "\n",
    "        label = y[node_idx]\n",
    "        if len(label.shape) == 0:\n",
    "            label = label.unsqueeze(dim=0)\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, self.num_hops, edge_index,\n",
    "                            relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "        steps = 40\n",
    "        self.model.eval()\n",
    "        grads = torch.zeros(steps+1, x.shape[1]).to(device)\n",
    "\n",
    "        # Perform Riemannian integration\n",
    "        for i in range(steps+1):\n",
    "            with torch.no_grad():\n",
    "                baseline = torch.zeros_like(sub_x).to(device)  # TODO: baseline all 0s, all 1s, ...?\n",
    "                temp_x = baseline + (float(i)/steps) * (sub_x.clone()-baseline)\n",
    "            temp_x.requires_grad = True\n",
    "            output = self.model(temp_x, sub_edge_index)\n",
    "            loss = self.criterion(output[mapping], label)\n",
    "            loss.backward()\n",
    "            grad = temp_x.grad[torch.where(subset==node_idx)[0].item()]\n",
    "            grads[i] = grad\n",
    "\n",
    "        grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "        avg_grads = torch.mean(grads, axis=0)\n",
    "\n",
    "        # Integrated gradients for only node_idx:\n",
    "        # baseline[0] just gets a single-value 0-tensor\n",
    "        integrated_gradients = ((x[torch.where(subset == node_idx)[0].item()]\n",
    "                                    - baseline[0]) * avg_grads)\n",
    "\n",
    "        # Integrated gradients across the enclosing subgraph:\n",
    "        all_node_ig = ((x[subset] - baseline) * avg_grads)\n",
    "        node_importances = torch.sum(all_node_ig, dim=1)\n",
    "        exp = Explanation(\n",
    "            feature_imp = integrated_gradients,\n",
    "            node_imp = node_importances,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "        gc.collect()\n",
    "        return exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sufficiency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, sys\n",
    "from typing import Union, List\n",
    "\n",
    "from networkx.classes.function import to_undirected\n",
    "import networkx as nx\n",
    "import ipdb\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from numpy import ndarray\n",
    "from torch_geometric.utils import to_networkx\n",
    "import gc\n",
    "\n",
    "from graphxai.explainers import CAM, GradCAM, GNNExplainer\n",
    "# from graphxai.explainers.utils.visualizations import visualize_subgraph_explanation\n",
    "from graphxai.visualization.visualizations import visualize_subgraph_explanation\n",
    "from graphxai.visualization.explanation_vis import visualize_node_explanation\n",
    "from graphxai.gnn_models.node_classification import GCN, train, test\n",
    "from graphxai.gnn_models.node_classification.testing import GCN_3layer_basic, GIN_3layer_basic\n",
    "\n",
    "from graphxai.gnn_models.node_classification import GCN, train, test\n",
    "from graphxai.gnn_models.node_classification.testing import GCN_3layer_basic, train, test\n",
    "\n",
    "from graphxai.datasets.shape_graph import ShapeGGen\n",
    "from graphxai.utils import to_networkx_conv, Explanation, distance\n",
    "from graphxai.utils.perturb import rewire_edges\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tqdm import tqdm\n",
    "\n",
    "B = {1,5,10,20,50}\n",
    "len_b = len(B)\n",
    "def sufficiency(generated_exp: list, shape_graph: any, model: torch.nn.Module) -> list:\n",
    "    '''\n",
    "    Args:\n",
    "        gt_exp (list): Ground truth explanation from the dataset.\n",
    "        generated_exp (Explanation): Explanation output by an explainer.\n",
    "    '''\n",
    "\n",
    "    # TODO: 1) Implement perturbations for continuous and discrete node attribute features\n",
    "    suff = list() # GEF = 1/m * sum(||F(x)-F(y)||) where F(x) is the original importances and F(y) is the perturbed\n",
    "\n",
    "    # Accessing the enclosing subgraph. Will be the same for both explanation.:\n",
    "    with tqdm(total=len(generated_exp)) as pbar:\n",
    "        for exp in generated_exp:\n",
    "            pbar.update(1)\n",
    "            exp_subgraph = exp.enc_subgraph\n",
    "            dist_total = 0\n",
    "            for q in B:\n",
    "                top_k = q / 100 # getting top k percent\n",
    "                # Identifying the top_k nodes in the explanation subgraph\n",
    "                top_k_nodes = exp.node_imp.topk(int(exp.node_imp.shape[0] * top_k))[1]\n",
    "                rem_nodes = []\n",
    "                for node in range(exp.node_imp.shape[0]):\n",
    "                    if node not in top_k_nodes:\n",
    "                        rem_nodes.append([k for k, v in exp.node_reference.items() if v == node][0])\n",
    "\n",
    "                # Getting the softmax vector for the original graph\n",
    "                org_vec = model(shape_graph.x, shape_graph.edge_index)[exp.node_idx]\n",
    "                org_softmax = F.softmax(org_vec, dim=-1)\n",
    "                # Getting the softmax vector for the perturbed graph\n",
    "                pert_x = shape_graph.x.clone()\n",
    "\n",
    "                # Removing the unimportant nodes by masking\n",
    "                pert_x[rem_nodes] = torch.zeros_like(pert_x[rem_nodes])  # torch.normal(0, 0.1, pert_x[rem_nodes].shape)\n",
    "                pert_vec = model(pert_x, shape_graph.edge_index)[exp.node_idx]\n",
    "\n",
    "                # check if the graph is disconnected!!\n",
    "                pert_softmax = F.softmax(pert_vec, dim=-1)\n",
    "\n",
    "                # summing the differences between the two explanations\n",
    "                dist = euclidean(org_softmax.detach().cpu().numpy(), pert_softmax.detach().cpu().numpy())\n",
    "\n",
    "                dist_total += dist\n",
    "                gc.collect()\n",
    "            suff.append(dist_total / len_b)\n",
    "    return suff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbp_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    bp_exps = list()\n",
    "    explainability_model = GuidedBP(model=model)\n",
    "    model.eval()\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = explainability_model.get_explanation_node(node_idx=node, x=data_object.x, edge_index=data_object.edge_index, y=data_object.y)\n",
    "            bp_exps.append(exp)\n",
    "    return bp_exps\n",
    "\n",
    "def cam_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    cam_exps = list()\n",
    "    model.eval()\n",
    "    explainability_model = CAM(model=model)\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = explainability_model.get_explanation_node(node_idx=node, x = data_object.x, edge_index = data_object.edge_index,  y = data_object.y)\n",
    "            cam_exps.append(exp)\n",
    "    return cam_exps\n",
    "    \n",
    "def ig_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    explanations_list = list()\n",
    "    model.eval()\n",
    "    ig = IG(model=model, criterion=torch.nn.CrossEntropyLoss(), hops=1)\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = ig.node_explanations(node_idx=node, x=data_object.x, edge_index=data_object.edge_index, y=data_object.y)\n",
    "            explanations_list.append(exp)\n",
    "    return explanations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "cam_sufficiency_list = list()\n",
    "loss_list = list()\n",
    "f1_accuracy_list = list()\n",
    "with tqdm(total=100) as pbar:\n",
    "    for epoch in range(100):\n",
    "        pbar.update(1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        # getting GuidedBP Explanations\n",
    "        # gbp = GuidedBP(model=model)\n",
    "        # getting IG explanations\n",
    "        cam_explanations = cam_exps(model, graph, data)\n",
    "\n",
    "        cam_suff = median(sufficiency(generated_exp=cam_explanations, shape_graph=data, model=model))\n",
    "        loss.add(cam_suff)\n",
    "\n",
    "        loss.backward()\n",
    "        loss_list.append(loss)\n",
    "        cam_sufficiency_list.append(cam_suff)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "        f1 = multiclass_f1_score(pred, data.y, num_classes=dataset.num_classes)\n",
    "        f1_accuracy_list.append(f1)\n",
    "        if(epoch % 25 == 0):\n",
    "            torch.save(model, \"sufficiency_100_cora_CAM.pt\")\n",
    "            df = pd.DataFrame([(loss_list[i].item(), cam_sufficiency_list[i], f1_accuracy_list[i].item()) for i in range(len(loss_list))], columns=[\"Loss\", \"CAM Sufficiency\", \"F1 Accuracy\"])\n",
    "            df.to_csv(\"Training_for_CAM_sufficiency_CORA\")\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving values to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"sufficiency_100_cora_CAM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22bf3b7c040>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i.item() for i in loss_list], label=\"Loss\")\n",
    "plt.plot([i for i in cam_sufficiency_list], label=\"Sufficiency\")\n",
    "plt.plot([i.item() for i in f1_accuracy_list], label=\"Macro F1\")\n",
    "plt.title(\"Loss, Sufficiency & Accuracy vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([(loss_list[i].item(), cam_sufficiency_list[i], f1_accuracy_list[i].item()) for i in range(len(loss_list))], columns=[\"Loss\", \"CAM Sufficiency\", \"F1 Accuracy\"])\n",
    "df.to_csv(\"Training_for_CAM_sufficiency_CORA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbp_sparsity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m cam \u001b[38;5;241m=\u001b[39m CAM(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# getting explanations for each model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m gbp_spar \u001b[38;5;241m=\u001b[39m \u001b[43mgbp_sparsity\u001b[49m(model, graph, data, gbp)\n\u001b[0;32m      6\u001b[0m cam_spar \u001b[38;5;241m=\u001b[39m cam_sparsity(model, graph, data, gbp)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGBP Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgbp_spar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCAM Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcam_spar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gbp_sparsity' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "cam = CAM(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_spar = gbp_sparsity(model, graph, data, gbp)\n",
    "cam_spar = cam_sparsity(model, graph, data, gbp)\n",
    "print(f\"GBP Sparsity: {gbp_spar}\\nCAM Sparsity: {cam_spar}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2708 [00:00<07:28,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6574, -0.7225, -0.6013, -0.5233], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2708 [00:00<08:56,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5203, -0.3700, -0.7551, -0.9783], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2708 [00:00<09:50,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8562, -0.5766, -0.4730, -1.1368, -0.3823,  0.0726], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2708 [00:01<10:19,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1077, -0.7583], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2708 [00:01<11:07,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5199, -0.6340, -0.2173, -0.2145, -0.0319, -0.6756], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2708 [00:01<11:24,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4461, -0.9948, -0.4571, -0.6390], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2708 [00:01<11:33,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4153, -0.4828, -0.5654, -0.5037, -0.5363], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2708 [00:02<11:29,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7944, -0.2029], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2708 [00:02<11:34,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6189, -0.9601, -0.6968, -0.6772], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/2708 [00:02<11:33,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1621, -0.9223, -0.8898], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/2708 [00:02<11:29,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8606, -0.7249, -0.6413], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/2708 [00:03<11:26,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3526, -0.1813, -0.2870], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/2708 [00:03<11:23,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4200, -0.2410, -0.5283, -0.3195, -0.4002], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2708 [00:03<11:19,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0116, -0.7319, -0.6148], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2708 [00:03<11:23,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8901, -0.2864,  0.0708, -0.5744, -0.2016, -0.7190], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2708 [00:04<11:21,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4788, -0.2618, -0.4735, -0.5339, -0.3104], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2708 [00:04<11:25,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9772, -0.3799, -0.5573, -0.5985, -0.6028], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2708 [00:04<11:27,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4950, -0.8515, -0.5688, -0.4359, -0.5582, -0.3338], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2708 [00:04<11:27,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3018, -1.0444, -0.6570, -0.8701, -0.1670, -0.8772], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2708 [00:05<11:25,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0686, -0.9931], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2708 [00:05<11:26,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3826,  0.0631, -0.8968, -0.9487, -0.6614, -0.3889], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/2708 [00:05<11:23,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6130,  0.0976, -0.6573], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2708 [00:05<11:31,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3956, -0.7277, -0.6689, -0.2478, -0.2256, -0.1082], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/2708 [00:06<11:36,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4049, -1.6713], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/2708 [00:06<11:32,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3425, -0.5798, -0.1702, -0.2315, -0.4489, -0.2593, -0.5921, -0.3989],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/2708 [00:06<11:36,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7113, -0.5180, -0.6471, -0.6454, -0.7941], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 28/2708 [00:07<11:32,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5213, -0.5076, -0.3410, -0.5158, -0.4208, -0.0247], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/2708 [00:07<11:36,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7930, -0.5129, -0.4541, -0.4210, -0.4524], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/2708 [00:07<11:46,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2461, -0.7360], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/2708 [00:07<11:57,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2923, -1.5444, -0.3596], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 32/2708 [00:08<12:08,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9600, -0.5965, -0.4892, -0.4216,  0.2531, -0.5733, -0.7309],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33/2708 [00:08<11:52,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1677, -0.9148], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 34/2708 [00:08<11:49,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0727, -0.4356, -0.0719, -0.8802, -0.3891], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 35/2708 [00:08<11:56,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6708, -0.2133, -0.4639, -0.0452, -0.1081, -0.2871, -0.5587, -0.3957,\n",
      "        -0.1813, -0.5420], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 36/2708 [00:09<11:49,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9400, -1.0512], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 37/2708 [00:09<11:40,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4529, -0.4503, -0.3648, -0.6088], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 38/2708 [00:09<11:38,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5024, -0.2346, -0.5609, -0.3737, -0.3356, -0.3844, -0.2932, -0.3747,\n",
      "        -0.2052], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 39/2708 [00:09<11:43,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3181, -0.1349, -0.4077, -0.1352], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 40/2708 [00:10<11:49,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9254, -0.6111, -0.1048, -0.3196, -0.2110], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 41/2708 [00:10<11:39,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4832, -0.8994, -0.2025, -0.5024, -0.3493, -0.3469, -0.3100, -0.1292],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 42/2708 [00:10<11:35,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9360, -0.1485, -0.8748, -0.8013], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 43/2708 [00:10<11:30,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5408, -0.5323, -0.5018, -0.2520, -0.5001], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 44/2708 [00:11<11:30,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9175, -0.6259, -0.8920], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 45/2708 [00:11<11:29,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5529, -0.4633, -0.6824, -0.1575, -0.6823, -0.4527, -0.1097],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 46/2708 [00:11<11:26,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7863, -0.6055, -0.3041, -0.7659], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 47/2708 [00:12<11:24,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6369, -0.1579,  0.0096, -0.2302, -0.7066, -0.8424, -0.8205],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 48/2708 [00:12<11:23,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5954, -0.7689, -1.0760], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 49/2708 [00:12<11:25,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6883, -0.7915, -0.8190], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 50/2708 [00:12<11:31,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7514, -0.5609, -0.5929, -0.4977, -0.6895, -0.1147, -0.3839, -0.4977,\n",
      "        -0.3521, -0.1876], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 51/2708 [00:13<11:27,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9633, -0.3087, -0.7496], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 52/2708 [00:13<11:32,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0716, -0.7867], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 53/2708 [00:13<11:26,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8026, -0.6366, -0.5278, -0.3520, -0.0046, -0.2991, -0.3643],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 54/2708 [00:13<11:26,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2594, -0.6112, -0.5430, -0.0218, -0.8436, -0.6938], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 55/2708 [00:14<11:26,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6441, -0.4541, -0.7002, -0.3384], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 56/2708 [00:14<11:24,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5803, -0.7055, -1.0509], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 57/2708 [00:14<11:28,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9221,  0.3088, -0.4489, -0.3684, -0.2524, -0.0571, -0.5076, -0.1804,\n",
      "        -0.2160, -0.0491, -0.4002, -0.8905,  0.1593], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 58/2708 [00:14<11:34,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4749, -0.3756, -0.3725, -0.5048, -0.4532], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 59/2708 [00:15<11:46,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2448, -0.8058], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 60/2708 [00:15<11:38,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3131, -1.6650], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 61/2708 [00:15<11:45,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4681, -0.5944, -0.3451, -0.4504, -0.3806, -0.3288, -0.3684, -0.7651,\n",
      "        -0.3525, -0.0654, -0.5855], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 62/2708 [00:15<11:42,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1158,  0.6875, -1.3673, -0.8284], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 63/2708 [00:16<11:49,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6049, -0.1793, -0.9846, -0.9551, -0.0148, -0.7559], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 64/2708 [00:16<11:47,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4189, -1.4566], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 65/2708 [00:16<11:40,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1445, -0.6135], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 66/2708 [00:17<11:26,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6229, -0.6396, -0.7113, -0.1714], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ig_spar \u001b[38;5;241m=\u001b[39m \u001b[43mig_sparsity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIG Spar: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mig_spar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m, in \u001b[0;36mig_sparsity\u001b[1;34m(model, graph, data_object)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[0;32m     33\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mIG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     total_sparse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count_nonzeros(exp\u001b[38;5;241m.\u001b[39mnode_imp)\u001b[38;5;241m/\u001b[39mexp\u001b[38;5;241m.\u001b[39menc_subgraph\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exp\u001b[38;5;241m.\u001b[39mnode_imp)\n",
      "Cell \u001b[1;32mIn[3], line 1020\u001b[0m, in \u001b[0;36mIG\u001b[1;34m(node_idx, x, edge_index, y, num_hops, steps, model, criterion)\u001b[0m\n\u001b[0;32m   1018\u001b[0m output \u001b[38;5;241m=\u001b[39m model(temp_x, sub_edge_index)\n\u001b[0;32m   1019\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[mapping], label)\n\u001b[1;32m-> 1020\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m grad \u001b[38;5;241m=\u001b[39m temp_x\u001b[38;5;241m.\u001b[39mgrad[torch\u001b[38;5;241m.\u001b[39mwhere(subset\u001b[38;5;241m==\u001b[39mnode_idx)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[0;32m   1022\u001b[0m grads[i] \u001b[38;5;241m=\u001b[39m grad\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\mainEnv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\mainEnv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ig_spar = ig_sparsity(model, graph, data)\n",
    "\n",
    "print(f\"IG Spar: {ig_spar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8040\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2708/2708 [04:38<00:00,  9.73it/s]\n",
      "100%|| 2708/2708 [12:55<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Sparsity: 1.0\n",
      "CAM Sparsity: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "cam = CAM(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_spar = gbp_sparsity(model, graph, data, gbp)\n",
    "cam_spar = cam_sparsity(model, graph, data, gbp)\n",
    "print(f\"GBP Sparsity: {gbp_spar}\\nCAM Sparsity: {cam_spar}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2708/2708 [11:26<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Spar: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ig_spar = ig_sparsity(model, graph, data)\n",
    "\n",
    "print(f\"IG Spar: {ig_spar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2708/2708 [12:32:29<00:00, 16.67s/it]        \n",
      "100%|| 2708/2708 [17:32<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Sparsity: 1.0\n",
      "CAM Sparsity: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "cam = CAM(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_spar = gbp_sparsity(model, graph, data, gbp)\n",
    "cam_spar = cam_sparsity(model, graph, data, gbp)\n",
    "print(f\"GBP Sparsity: {gbp_spar}\\nCAM Sparsity: {cam_spar}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2708/2708 [10:33<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Spar: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ig_spar = ig_sparsity(model, graph, data)\n",
    "\n",
    "print(f\"IG Spar: {ig_spar}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
