{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook gets the baselines for MUTAG with Sufficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://drive.usercontent.google.com/download?id=12aJWAGCM4IvdGI2fiydDNyWzViEOLZH8&confirm=t\n",
      "Extracting \\root\\facebook\\raw\\data.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import AttributedGraphDataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from graphxai.datasets import MUTAG\n",
    "\n",
    "dataset = AttributedGraphDataset(root='/root/', name='Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import k_hop_subgraph, to_undirected\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.data import Data\n",
    "from graphxai.explainers._base import _BaseExplainer\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class _BaseExplainer:\n",
    "    \"\"\"\n",
    "    Base Class for Explainers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            model: nn.Module,\n",
    "            emb_layer_name: Optional[str] = None,\n",
    "            is_subgraphx: Optional[bool] = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "                The output of the model should be unnormalized class score.\n",
    "                For example, last layer = GCNConv or Linear.\n",
    "            emb_layer_name (str, optional): name of the embedding layer\n",
    "                If not specified, use the last but one layer by default.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.L = len([module for module in self.model.modules()\n",
    "                      if isinstance(module, MessagePassing)])\n",
    "        self.explain_graph = False  # Assume node-level explanation by default\n",
    "        self.subgraphx_flag = is_subgraphx\n",
    "        self.__set_embedding_layer(emb_layer_name)\n",
    "\n",
    "    def __set_embedding_layer(self, emb_layer_name: str = None):\n",
    "        \"\"\"\n",
    "        Set the embedding layer (by default is the last but one layer).\n",
    "        \"\"\"\n",
    "        if emb_layer_name:\n",
    "            try:\n",
    "                self.emb_layer = getattr(self.model, emb_layer_name)\n",
    "            except AttributeError:\n",
    "                raise ValueError(f'{emb_layer_name} does not exist in the model')\n",
    "        else:\n",
    "            self.emb_layer = list(self.model.modules())[-2]\n",
    "\n",
    "    def _get_embedding(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                       forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the embedding.\n",
    "        \"\"\"\n",
    "        emb = self._get_activation(self.emb_layer, x, edge_index, forward_kwargs)\n",
    "        return emb\n",
    "\n",
    "    def _set_masks(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                   edge_mask: torch.Tensor = None, explain_feature: bool = False,\n",
    "                   device = None):\n",
    "        \"\"\"\n",
    "        Initialize the edge (and feature) masks.\n",
    "        \"\"\"\n",
    "        (n, d), m = x.shape, edge_index.shape[1]\n",
    "\n",
    "        # Initialize edge_mask and feature_mask for learning\n",
    "        std = torch.nn.init.calculate_gain('relu') * np.sqrt(2.0 / (2 * n))\n",
    "        if edge_mask is None:\n",
    "            edge_mask = (torch.randn(m) * std).to(device)\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        else:\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        if explain_feature:\n",
    "            feature_mask = (torch.randn(d) * 0.1).to(device)\n",
    "            self.feature_mask = torch.nn.Parameter(feature_mask)\n",
    "\n",
    "        self.loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "        # Tell pytorch geometric to apply edge masks\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "                module.__loop_mask__ = self.loop_mask\n",
    "\n",
    "    def _clear_masks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.edge_mask = None\n",
    "        self.feature_mask = None\n",
    "\n",
    "    def _flow(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def _predict(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                 return_type: str = 'label', forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the model's prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            return_type (str): one of ['label', 'prob', 'log_prob']\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            pred (torch.Tensor, [n x ...]): model prediction\n",
    "        \"\"\"\n",
    "        # Compute unnormalized class score\n",
    "        with torch.no_grad():\n",
    "            out = self.model.to(device)(x, edge_index, **forward_kwargs)\n",
    "            if return_type == 'label':\n",
    "                out = out.argmax(dim=-1)\n",
    "            elif return_type == 'prob':\n",
    "                out = F.softmax(out, dim=-1)\n",
    "            elif return_type == 'log_prob':\n",
    "                out = F.log_softmax(out, dim=-1)\n",
    "            else:\n",
    "                raise ValueError(\"return_type must be 'label', 'prob', or 'log_prob'\")\n",
    "\n",
    "            if self.explain_graph:\n",
    "                out = out.squeeze()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _prob_score_func_graph(self, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probability that the input graphs\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            target_class (int): the targeted class of the graph\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        def get_prob_score(x: torch.Tensor,\n",
    "                           edge_index: torch.Tensor,\n",
    "                           forward_kwargs: dict = {}):\n",
    "            prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                 forward_kwargs=forward_kwargs)\n",
    "            score = prob[:, target_class]\n",
    "            return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _prob_score_func_node(self, node_idx: torch.Tensor, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probabilities that k specified nodes\n",
    "        in `torch_geometric.data.Batch` (disconnected union of the input graphs)\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            node_idx (torch.Tensor, [k]): the indices of the k nodes interested\n",
    "            target_class (torch.Tensor, [k]): the targeted classes of the k nodes\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        if self.subgraphx_flag:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[node_idx, target_class]\n",
    "                return score\n",
    "        else:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[:, node_idx, target_class]\n",
    "                return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _get_activation(self, layer: nn.Module, x: torch.Tensor,\n",
    "                        edge_index: torch.Tensor, forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the activation of the layer.\n",
    "        \"\"\"\n",
    "        activation = {}\n",
    "        def get_activation():\n",
    "            def hook(model, inp, out):\n",
    "                activation['layer'] = out.detach()\n",
    "            return hook\n",
    "\n",
    "        layer.register_forward_hook(get_activation())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return activation['layer']\n",
    "\n",
    "    def _get_k_hop_subgraph(self, node_idx: int, x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor, num_hops: int = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Extract the subgraph of target node\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): the node index\n",
    "            x (torch.Tensor, [n x d]): node feature matrix with shape\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index\n",
    "            kwargs (dict): additional parameters of the graph\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # TODO: use NamedTuple\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        return khop_info\n",
    "\n",
    "    def get_explanation_node(self, node_idx: int,\n",
    "                             x: torch.Tensor,\n",
    "                             edge_index: torch.Tensor,\n",
    "                             label: torch.Tensor = None,\n",
    "                             num_hops: int = None,\n",
    "                             forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved\n",
    "        \"\"\"\n",
    "        # If labels are needed\n",
    "        label = self._predict(x, edge_index, return_type='label') if label is None else label\n",
    "        # If probabilities / log probabilities are needed\n",
    "        prob = self._predict(x, edge_index, return_type='prob')\n",
    "        log_prob = self._predict(x, edge_index, return_type='log_prob')\n",
    "\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return exp, khop_info\n",
    "\n",
    "    def get_explanation_graph(self, edge_index: torch.Tensor,\n",
    "                              x: torch.Tensor, label: torch.Tensor,\n",
    "                              forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a whole-graph prediction.\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            label (torch.Tensor, [n x ...]): labels to explain\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "        \"\"\"\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "from graphxai.utils import Explanation\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph as subgraph\n",
    "\n",
    "class GNNPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class _BaseDecomposition(_BaseExplainer):\n",
    "    '''\n",
    "    Code adapted from Dive into Graphs (DIG)\n",
    "    Code: https://github.com/divelab/DIG\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__(model=model) # Will set self.model = model\n",
    "        # Other properties: self.L (number of layers)\n",
    "\n",
    "    @property\n",
    "    def __num_hops__(self):\n",
    "        if self.explain_graph:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.L\n",
    "    \n",
    "    def set_graph_attr(self,\n",
    "                x: Tensor,\n",
    "                edge_index: Tensor,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.num_edges = edge_index.shape[1]\n",
    "        self.num_nodes = x.shape[0]\n",
    "        self.device = x.device\n",
    "\n",
    "    def extract_step(self, x: Tensor, edge_index: Tensor, detach: bool = True, split_fc: bool = False, forward_kwargs: dict = None):\n",
    "        '''Gets information about every layer in the graph\n",
    "        Args:\n",
    "\n",
    "            forward_kwargs (tuple, optional): Additional arguments to model forward call (other than x and edge_index)\n",
    "                (default: :obj:`None`)\n",
    "        '''\n",
    "\n",
    "        layer_extractor = []\n",
    "        hooks = []\n",
    "\n",
    "        def register_hook(module: nn.Module):\n",
    "            if not list(module.children()) or isinstance(module, MessagePassing):\n",
    "                hooks.append(module.register_forward_hook(forward_hook))\n",
    "\n",
    "        def forward_hook(module: nn.Module, input: Tuple[Tensor], output: Tensor):\n",
    "            # input contains x and edge_index\n",
    "            if detach:\n",
    "                layer_extractor.append((module, input[0].clone().detach(), output.clone().detach()))\n",
    "            else:\n",
    "                layer_extractor.append((module, input[0], output))\n",
    "\n",
    "        # --- register hooks ---\n",
    "        self.model.apply(register_hook)\n",
    "\n",
    "        # ADDED: OWEN QUEEN --------------\n",
    "        if forward_kwargs is None:\n",
    "            _ = self.model(x, edge_index)\n",
    "        else:\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "        # --------------------------------\n",
    "        # Remove hooks:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # --- divide layer sets ---\n",
    "\n",
    "        # print('Layer extractor', [layer_extractor[i][0] for i in range(len(layer_extractor))])\n",
    "\n",
    "        walk_steps = []\n",
    "        fc_steps = []\n",
    "        pool_flag = False\n",
    "        step = {'input': None, 'module': [], 'output': None}\n",
    "        for layer in layer_extractor:\n",
    "            if isinstance(layer[0], MessagePassing):\n",
    "                if step['module']: # Append step that had previously been building\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], GNNPool):\n",
    "                pool_flag = True\n",
    "                if step['module']:\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                # Putting in GNNPool\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], nn.Linear):\n",
    "                if step['module']:\n",
    "                    if isinstance(step['module'][0], MessagePassing):\n",
    "                        walk_steps.append(step) # Append MessagePassing layer to walk_steps\n",
    "                    else: # Always append Linear layers to fc_steps\n",
    "                        fc_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            # Also appends non-trainable layers to step (not modifying input):\n",
    "            step['module'].append(layer[0])\n",
    "            step['output'] = layer[2]\n",
    "\n",
    "        if step['module']:\n",
    "            if isinstance(step['module'][0], MessagePassing):\n",
    "                walk_steps.append(step)\n",
    "            else: # Append anything to FC that is not MessagePassing at its origin\n",
    "                # Still supports sequential layers\n",
    "                fc_steps.append(step)\n",
    "            # print('layer', layer[0])\n",
    "            # if isinstance(layer[0], MessagePassing) or isinstance(layer[0], GNNPool):\n",
    "            #     if isinstance(layer[0], GNNPool):\n",
    "            #         pool_flag = True\n",
    "            #     if step['module'] and step['input'] is not None:\n",
    "            #         walk_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # if pool_flag and split_fc and isinstance(layer[0], nn.Linear):\n",
    "            #     if step['module']:\n",
    "            #         fc_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # step['module'].append(layer[0])\n",
    "            # step['output'] = layer[2]\n",
    "\n",
    "        for walk_step in walk_steps:\n",
    "            if hasattr(walk_step['module'][0], 'nn') and walk_step['module'][0].nn is not None:\n",
    "                # We don't allow any outside nn during message flow process in GINs\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "            elif hasattr(walk_step['module'][0], 'lin') and walk_step['module'][0].lin is not None:\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "\n",
    "        # print('Walk steps', [walk_steps[i]['module'] for i in range(len(walk_steps))])\n",
    "        # print('fc steps', [fc_steps[i]['module'] for i in range(len(fc_steps))])\n",
    "\n",
    "        return walk_steps, fc_steps\n",
    "\n",
    "    def walks_pick(self,\n",
    "                   edge_index: Tensor,\n",
    "                   pick_edge_indices: List,\n",
    "                   walk_indices: List=[],\n",
    "                   num_layers=0\n",
    "                   ):\n",
    "        walk_indices_list = []\n",
    "        for edge_idx in pick_edge_indices:\n",
    "\n",
    "            # Adding one edge\n",
    "            walk_indices.append(edge_idx)\n",
    "            _, new_src = src, tgt = edge_index[:, edge_idx]\n",
    "            next_edge_indices = np.array((edge_index[0, :] == new_src).nonzero().view(-1))\n",
    "\n",
    "            # Finding next edge\n",
    "            if len(walk_indices) >= num_layers:\n",
    "                # return one walk\n",
    "                walk_indices_list.append(walk_indices.copy())\n",
    "            else:\n",
    "                walk_indices_list += self.walks_pick(edge_index, next_edge_indices, walk_indices, num_layers)\n",
    "\n",
    "            # remove the last edge\n",
    "            walk_indices.pop(-1)\n",
    "\n",
    "        return walk_indices_list\n",
    "\n",
    "class CAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, activation = None):\n",
    "        '''\n",
    "        .. note::\n",
    "            From Pope et al., CAM requires that the layer immediately before the softmax layer be\n",
    "            a global average pooling layer, or in the case of node classification, a graph convolutional\n",
    "            layer. Therefore, for this algorithm to theoretically work, there can be no fully-connected\n",
    "            layers after global pooling. There is no restriction in the code for this, but be warned. \n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            activation (method, optional): activation funciton for final layer in network. If `activation = None`,\n",
    "                explainer assumes linear activation. Use `activation = None` if the activation is applied\n",
    "                within the `forward` method of `model`, only set this parameter if another activation is\n",
    "                applied in the training procedure outside of model. (:default: :obj:`None`)\n",
    "        '''\n",
    "        super().__init__(model=model)\n",
    "        self.model = model\n",
    "\n",
    "        # Set activation function\n",
    "        self.activation = lambda x: x  if activation is None else activation\n",
    "        # i.e. linear activation if none provided\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "                x: torch.Tensor, \n",
    "                node_idx: int, \n",
    "                edge_index: torch.Tensor, \n",
    "                label: int = None,  \n",
    "                y = None,\n",
    "                forward_kwargs: dict = {},\n",
    "                directed: bool = False\n",
    "            ) -> Explanation:\n",
    "        '''\n",
    "        Explain one node prediction by the model\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of node features from the entire graph\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.tensor): edge_index of entire graph\n",
    "            label (int, optional): Label on which to compute the explanation for\n",
    "                this node. If `None`, the predicted label from the model will be\n",
    "                used. (default: :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "            directed (bool, optional): If True, graph is directed.\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        if not directed:\n",
    "            edge_index = to_undirected(edge_index)\n",
    "\n",
    "        if label is None:\n",
    "            if y is None:\n",
    "                label = int(self.__forward_pass(x, edge_index, forward_kwargs).argmax(dim=1).item())\n",
    "            else:\n",
    "                label = y[node_idx]\n",
    "\n",
    "        # Perform walk:\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=False, split_fc=False, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        # Get subgraph:\n",
    "        khop_info = k_hop_subgraph(node_idx = node_idx, num_hops = self.L, edge_index = edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        #cam = torch.zeros(N) # Compute CAM only over the subgraph (all others are zero)\n",
    "        cam = torch.zeros(subgraph_N)\n",
    "        for i in range(subgraph_N):\n",
    "            n = subgraph_nodes[i]\n",
    "            cam[i] += self.__exp_node(n, walk_steps, label)\n",
    "\n",
    "        # Set Explanation class:\n",
    "        exp = Explanation(\n",
    "            node_imp = cam,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def __forward_pass(self, x, edge_index, forward_kwargs = {}):\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, predicted_c):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after last convolutiuonal layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        last_conv_layer = walk_steps[-1]\n",
    "\n",
    "        if isinstance(last_conv_layer['module'][0], GINConv):\n",
    "            weight_vec = last_conv_layer['module'][0].nn.weight[predicted_c, :].detach()  # last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], GCNConv):\n",
    "            weight_vec = last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], torch.nn.Linear):\n",
    "            weight_vec = last_conv_layer['module'][0].weight[predicted_c, :].detach()\n",
    "\n",
    "        F_l_n = F.relu(last_conv_layer['input'][node_idx,:]).detach()\n",
    "\n",
    "        L_cam_n = F.relu(torch.matmul(weight_vec, F_l_n))\n",
    "\n",
    "        return L_cam_n.item()\n",
    "\n",
    "\n",
    "class GradCAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Gradient Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.Tensor, criterion = F.cross_entropy):\n",
    "        '''\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            criterion (PyTorch Loss Function): loss function used to train the model.\n",
    "                Needed to pass gradients backwards in the network to obtain gradients.\n",
    "        '''\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "            x: torch.Tensor, \n",
    "            y: torch.Tensor, \n",
    "            node_idx: int, \n",
    "            edge_index: torch.Tensor, \n",
    "            label: int = None, \n",
    "            forward_kwargs: dict = {}, \n",
    "            average_variant: bool = True, \n",
    "            layer: int = 0\n",
    "        ) -> Explanation:\n",
    "        '''\n",
    "        Explain a node in the given graph\n",
    "        Args:\n",
    "            x (torch.Tensor, (n,)): Tensor of node features from the entire graph, with n nodes.\n",
    "            y (torch.Tensor, (n,)): Ground-truth labels for all n nodes in the graph.\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.Tensor): Edge_index of entire graph.\n",
    "            label (int, optional): Label for which to compute Grad-CAM against. If None, computes\n",
    "                the Grad-CAM with respect to the model's predicted class for this node.\n",
    "                (default :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. (default: :obj:`None`)\n",
    "            average_variant (bool, optional): If True, computes the average Grad-CAM across all convolutional\n",
    "                layers in the model. If False, computes Grad-CAM for `layer`. (default: :obj:`True`)\n",
    "            layer (int, optional): Layer by which to compute the Grad-CAM. Argument only has an effect if \n",
    "                `average_variant == True`. Must be less-than the total number of convolutional layers\n",
    "                in the model. (default: :obj:`0`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        x = x.detach().clone()\n",
    "        y = y.detach().clone()\n",
    "\n",
    "        x.requires_grad = True\n",
    "\n",
    "        if label is None:\n",
    "            pred = self.__forward_pass(x, y, edge_index, forward_kwargs)[0][node_idx, :].reshape(1, -1)\n",
    "            y[node_idx] = pred.argmax(dim=1).item()\n",
    "        else: # Transform node_idx's label if provided by user\n",
    "            pred, loss = self.__forward_pass(x, y, edge_index, forward_kwargs)\n",
    "            y[node_idx] = label\n",
    "\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=True, split_fc=True, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        khop_info = k_hop_subgraph(node_idx, self.L, edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        if average_variant:\n",
    "            # Size of all nodes in the subgraph:\n",
    "            avg_gcam = torch.zeros(subgraph_N)\n",
    "\n",
    "            for l in range(self.L):\n",
    "                # Compute gradients for this layer ahead of time:\n",
    "                gradients = self.__grad_by_layer(l)\n",
    "\n",
    "                for i in range(subgraph_N): # Over all subgraph nodes\n",
    "                    n = subgraph_nodes[i]\n",
    "                    avg_gcam[i] += self.__get_gCAM_layer(walk_steps, l, n, gradients)\n",
    "\n",
    "            avg_gcam /= self.L # Apply average\n",
    "\n",
    "            exp.node_imp = avg_gcam\n",
    "\n",
    "        else:\n",
    "            assert layer < len(walk_steps), \"Layer must be an index of convolutional layers\"\n",
    "\n",
    "            gcam = torch.zeros(subgraph_N)\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "            for i in range(subgraph_N):\n",
    "                n = subgraph_nodes[i]\n",
    "                gcam[i] += self.__get_gCAM_layer(walk_steps, layer, n, gradients)#[0]\n",
    "\n",
    "            exp.node_imp = gcam\n",
    "\n",
    "        return exp\n",
    "        \n",
    "    def __forward_pass(self, x, label, edge_index, forward_kwargs):\n",
    "        x.requires_grad = True # Enforce that x needs gradient\n",
    "\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        loss = self.criterion(pred, label)\n",
    "        loss.backward() # Propagate loss backward through network\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def __grad_by_layer(self, layer):\n",
    "        # Index 0 of parameters to avoid calculating gradients for biases\n",
    "        module_at_layer = list(self.model.children())[layer]\n",
    "\n",
    "        if isinstance(module_at_layer, GCNConv):\n",
    "            grad = module_at_layer.lin.weight.grad\n",
    "        elif isinstance(module_at_layer, GINConv):\n",
    "            grad = module_at_layer.nn.weight.grad\n",
    "        else:\n",
    "            grad = module_at_layer.weight.grad\n",
    "\n",
    "        return grad.mean(dim=1)\n",
    "\n",
    "    def __get_gCAM_layer(self, walk_steps, layer, node_idx = None, gradients = None):\n",
    "        # Gets Grad CAM for one layer\n",
    "        if gradients is None:\n",
    "            # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "            # ''        '' shape: [k,] - k= # output features from layer l\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "\n",
    "        if node_idx is None: # Need to compute for entire graph:\n",
    "            node_explanations = []\n",
    "            for n in range(self.N):\n",
    "                node_explanations.append(self.__exp_node(n, walk_steps, layer, gradients))\n",
    "\n",
    "            return node_explanations\n",
    "\n",
    "        # Return for only one node:\n",
    "        return self.__exp_node(node_idx, walk_steps, layer, gradients)\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, layer, gradients):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after each convolutional layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "        # ''        '' shape: [k,] - k= # input features to layer l\n",
    "\n",
    "        # Activations for node n\n",
    "        F_l_n = F.relu(walk_steps[layer]['output'][node_idx,:]).detach()\n",
    "        L_cam_n = F.relu(torch.matmul(gradients, F_l_n)) # Combine gradients and activations\n",
    "\n",
    "        return L_cam_n.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.utils import k_hop_subgraph, to_undirected\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.data import Data\n",
    "from graphxai.explainers._base import _BaseExplainer\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class _BaseExplainer:\n",
    "    \"\"\"\n",
    "    Base Class for Explainers\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            model: nn.Module,\n",
    "            emb_layer_name: Optional[str] = None,\n",
    "            is_subgraphx: Optional[bool] = False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "                The output of the model should be unnormalized class score.\n",
    "                For example, last layer = GCNConv or Linear.\n",
    "            emb_layer_name (str, optional): name of the embedding layer\n",
    "                If not specified, use the last but one layer by default.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.L = len([module for module in self.model.modules()\n",
    "                      if isinstance(module, MessagePassing)])\n",
    "        self.explain_graph = False  # Assume node-level explanation by default\n",
    "        self.subgraphx_flag = is_subgraphx\n",
    "        self.__set_embedding_layer(emb_layer_name)\n",
    "\n",
    "    def __set_embedding_layer(self, emb_layer_name: str = None):\n",
    "        \"\"\"\n",
    "        Set the embedding layer (by default is the last but one layer).\n",
    "        \"\"\"\n",
    "        if emb_layer_name:\n",
    "            try:\n",
    "                self.emb_layer = getattr(self.model, emb_layer_name)\n",
    "            except AttributeError:\n",
    "                raise ValueError(f'{emb_layer_name} does not exist in the model')\n",
    "        else:\n",
    "            self.emb_layer = list(self.model.modules())[-2]\n",
    "\n",
    "    def _get_embedding(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                       forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the embedding.\n",
    "        \"\"\"\n",
    "        emb = self._get_activation(self.emb_layer, x, edge_index, forward_kwargs)\n",
    "        return emb\n",
    "\n",
    "    def _set_masks(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                   edge_mask: torch.Tensor = None, explain_feature: bool = False,\n",
    "                   device = None):\n",
    "        \"\"\"\n",
    "        Initialize the edge (and feature) masks.\n",
    "        \"\"\"\n",
    "        (n, d), m = x.shape, edge_index.shape[1]\n",
    "\n",
    "        # Initialize edge_mask and feature_mask for learning\n",
    "        std = torch.nn.init.calculate_gain('relu') * np.sqrt(2.0 / (2 * n))\n",
    "        if edge_mask is None:\n",
    "            edge_mask = (torch.randn(m) * std).to(device)\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        else:\n",
    "            self.edge_mask = torch.nn.Parameter(edge_mask)\n",
    "        if explain_feature:\n",
    "            feature_mask = (torch.randn(d) * 0.1).to(device)\n",
    "            self.feature_mask = torch.nn.Parameter(feature_mask)\n",
    "\n",
    "        self.loop_mask = edge_index[0] != edge_index[1]\n",
    "\n",
    "        # Tell pytorch geometric to apply edge masks\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "                module.__loop_mask__ = self.loop_mask\n",
    "\n",
    "    def _clear_masks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.edge_mask = None\n",
    "        self.feature_mask = None\n",
    "\n",
    "    def _flow(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def _predict(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                 return_type: str = 'label', forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the model's prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            return_type (str): one of ['label', 'prob', 'log_prob']\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            pred (torch.Tensor, [n x ...]): model prediction\n",
    "        \"\"\"\n",
    "        # Compute unnormalized class score\n",
    "        with torch.no_grad():\n",
    "            out = self.model.to(device)(x, edge_index, **forward_kwargs)\n",
    "            if return_type == 'label':\n",
    "                out = out.argmax(dim=-1)\n",
    "            elif return_type == 'prob':\n",
    "                out = F.softmax(out, dim=-1)\n",
    "            elif return_type == 'log_prob':\n",
    "                out = F.log_softmax(out, dim=-1)\n",
    "            else:\n",
    "                raise ValueError(\"return_type must be 'label', 'prob', or 'log_prob'\")\n",
    "\n",
    "            if self.explain_graph:\n",
    "                out = out.squeeze()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _prob_score_func_graph(self, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probability that the input graphs\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            target_class (int): the targeted class of the graph\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        def get_prob_score(x: torch.Tensor,\n",
    "                           edge_index: torch.Tensor,\n",
    "                           forward_kwargs: dict = {}):\n",
    "            prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                 forward_kwargs=forward_kwargs)\n",
    "            score = prob[:, target_class]\n",
    "            return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _prob_score_func_node(self, node_idx: torch.Tensor, target_class: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get a function that computes the predicted probabilities that k specified nodes\n",
    "        in `torch_geometric.data.Batch` (disconnected union of the input graphs)\n",
    "        are classified as target classes.\n",
    "\n",
    "        Args:\n",
    "            node_idx (torch.Tensor, [k]): the indices of the k nodes interested\n",
    "            target_class (torch.Tensor, [k]): the targeted classes of the k nodes\n",
    "\n",
    "        Returns:\n",
    "            get_prob_score (callable): the probability score function\n",
    "        \"\"\"\n",
    "        if self.subgraphx_flag:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[node_idx, target_class]\n",
    "                return score\n",
    "        else:\n",
    "            def get_prob_score(x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor,\n",
    "                            forward_kwargs: dict = {}):\n",
    "                prob = self._predict(x, edge_index, return_type='prob',\n",
    "                                    forward_kwargs=forward_kwargs)\n",
    "                score = prob[:, node_idx, target_class]\n",
    "                return score\n",
    "\n",
    "        return get_prob_score\n",
    "\n",
    "    def _get_activation(self, layer: nn.Module, x: torch.Tensor,\n",
    "                        edge_index: torch.Tensor, forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Get the activation of the layer.\n",
    "        \"\"\"\n",
    "        activation = {}\n",
    "        def get_activation():\n",
    "            def hook(model, inp, out):\n",
    "                activation['layer'] = out.detach()\n",
    "            return hook\n",
    "\n",
    "        layer.register_forward_hook(get_activation())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return activation['layer']\n",
    "\n",
    "    def _get_k_hop_subgraph(self, node_idx: int, x: torch.Tensor,\n",
    "                            edge_index: torch.Tensor, num_hops: int = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Extract the subgraph of target node\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): the node index\n",
    "            x (torch.Tensor, [n x d]): node feature matrix with shape\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index\n",
    "            kwargs (dict): additional parameters of the graph\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # TODO: use NamedTuple\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        return khop_info\n",
    "\n",
    "    def get_explanation_node(self, node_idx: int,\n",
    "                             x: torch.Tensor,\n",
    "                             edge_index: torch.Tensor,\n",
    "                             label: torch.Tensor = None,\n",
    "                             num_hops: int = None,\n",
    "                             forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a node prediction.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): index of the node to be explained\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            label (torch.Tensor, optional, [n x ...]): labels to explain\n",
    "                If not provided, we use the output of the model.\n",
    "            num_hops (int, optional): number of hops to consider\n",
    "                If not provided, we use the number of graph layers of the GNN.\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "            khop_info (4-tuple of torch.Tensor):\n",
    "                0. the nodes involved in the subgraph\n",
    "                1. the filtered `edge_index`\n",
    "                2. the mapping from node indices in `node_idx` to their new location\n",
    "                3. the `edge_index` mask indicating which edges were preserved\n",
    "        \"\"\"\n",
    "        # If labels are needed\n",
    "        label = self._predict(x, edge_index, return_type='label') if label is None else label\n",
    "        # If probabilities / log probabilities are needed\n",
    "        prob = self._predict(x, edge_index, return_type='prob')\n",
    "        log_prob = self._predict(x, edge_index, return_type='log_prob')\n",
    "\n",
    "        num_hops = self.L if num_hops is None else num_hops\n",
    "\n",
    "        khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "            k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                           relabel_nodes=True, num_nodes=x.shape[0])\n",
    "        sub_x = x[subset]\n",
    "\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return exp, khop_info\n",
    "\n",
    "    def get_explanation_graph(self, edge_index: torch.Tensor,\n",
    "                              x: torch.Tensor, label: torch.Tensor,\n",
    "                              forward_kwargs: dict = {}):\n",
    "        \"\"\"\n",
    "        Explain a whole-graph prediction.\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor, [2 x m]): edge index of the graph\n",
    "            x (torch.Tensor, [n x d]): node features\n",
    "            label (torch.Tensor, [n x ...]): labels to explain\n",
    "            forward_kwargs (dict, optional): additional arguments to model.forward\n",
    "                beyond x and edge_index\n",
    "\n",
    "        Returns:\n",
    "            exp (dict):\n",
    "                exp['feature_imp'] (torch.Tensor, [d]): feature mask explanation\n",
    "                exp['edge_imp'] (torch.Tensor, [m]): k-hop edge importance\n",
    "                exp['node_imp'] (torch.Tensor, [m]): k-hop node importance\n",
    "        \"\"\"\n",
    "        exp = {'feature_imp': None, 'edge_imp': None}\n",
    "\n",
    "        # Compute exp\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_explanation_link(self):\n",
    "        \"\"\"\n",
    "        Explain a link prediction.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "from graphxai.utils import Explanation\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph as subgraph\n",
    "\n",
    "class GNNPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class _BaseDecomposition(_BaseExplainer):\n",
    "    '''\n",
    "    Code adapted from Dive into Graphs (DIG)\n",
    "    Code: https://github.com/divelab/DIG\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__(model=model) # Will set self.model = model\n",
    "        # Other properties: self.L (number of layers)\n",
    "\n",
    "    @property\n",
    "    def __num_hops__(self):\n",
    "        if self.explain_graph:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.L\n",
    "    \n",
    "    def set_graph_attr(self,\n",
    "                x: Tensor,\n",
    "                edge_index: Tensor,\n",
    "                **kwargs\n",
    "                ):\n",
    "        self.num_edges = edge_index.shape[1]\n",
    "        self.num_nodes = x.shape[0]\n",
    "        self.device = x.device\n",
    "\n",
    "    def extract_step(self, x: Tensor, edge_index: Tensor, detach: bool = True, split_fc: bool = False, forward_kwargs: dict = None):\n",
    "        '''Gets information about every layer in the graph\n",
    "        Args:\n",
    "\n",
    "            forward_kwargs (tuple, optional): Additional arguments to model forward call (other than x and edge_index)\n",
    "                (default: :obj:`None`)\n",
    "        '''\n",
    "\n",
    "        layer_extractor = []\n",
    "        hooks = []\n",
    "\n",
    "        def register_hook(module: nn.Module):\n",
    "            if not list(module.children()) or isinstance(module, MessagePassing):\n",
    "                hooks.append(module.register_forward_hook(forward_hook))\n",
    "\n",
    "        def forward_hook(module: nn.Module, input: Tuple[Tensor], output: Tensor):\n",
    "            # input contains x and edge_index\n",
    "            if detach:\n",
    "                layer_extractor.append((module, input[0].clone().detach(), output.clone().detach()))\n",
    "            else:\n",
    "                layer_extractor.append((module, input[0], output))\n",
    "\n",
    "        # --- register hooks ---\n",
    "        self.model.apply(register_hook)\n",
    "\n",
    "        # ADDED: OWEN QUEEN --------------\n",
    "        if forward_kwargs is None:\n",
    "            _ = self.model(x, edge_index)\n",
    "        else:\n",
    "            _ = self.model(x, edge_index, **forward_kwargs)\n",
    "        # --------------------------------\n",
    "        # Remove hooks:\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # --- divide layer sets ---\n",
    "\n",
    "        # print('Layer extractor', [layer_extractor[i][0] for i in range(len(layer_extractor))])\n",
    "\n",
    "        walk_steps = []\n",
    "        fc_steps = []\n",
    "        pool_flag = False\n",
    "        step = {'input': None, 'module': [], 'output': None}\n",
    "        for layer in layer_extractor:\n",
    "            if isinstance(layer[0], MessagePassing):\n",
    "                if step['module']: # Append step that had previously been building\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], GNNPool):\n",
    "                pool_flag = True\n",
    "                if step['module']:\n",
    "                    walk_steps.append(step)\n",
    "\n",
    "                # Putting in GNNPool\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            elif isinstance(layer[0], nn.Linear):\n",
    "                if step['module']:\n",
    "                    if isinstance(step['module'][0], MessagePassing):\n",
    "                        walk_steps.append(step) # Append MessagePassing layer to walk_steps\n",
    "                    else: # Always append Linear layers to fc_steps\n",
    "                        fc_steps.append(step)\n",
    "\n",
    "                step = {'input': layer[1], 'module': [], 'output': None}\n",
    "\n",
    "            # Also appends non-trainable layers to step (not modifying input):\n",
    "            step['module'].append(layer[0])\n",
    "            step['output'] = layer[2]\n",
    "\n",
    "        if step['module']:\n",
    "            if isinstance(step['module'][0], MessagePassing):\n",
    "                walk_steps.append(step)\n",
    "            else: # Append anything to FC that is not MessagePassing at its origin\n",
    "                # Still supports sequential layers\n",
    "                fc_steps.append(step)\n",
    "            # print('layer', layer[0])\n",
    "            # if isinstance(layer[0], MessagePassing) or isinstance(layer[0], GNNPool):\n",
    "            #     if isinstance(layer[0], GNNPool):\n",
    "            #         pool_flag = True\n",
    "            #     if step['module'] and step['input'] is not None:\n",
    "            #         walk_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # if pool_flag and split_fc and isinstance(layer[0], nn.Linear):\n",
    "            #     if step['module']:\n",
    "            #         fc_steps.append(step)\n",
    "            #     step = {'input': layer[1], 'module': [], 'output': None}\n",
    "            # step['module'].append(layer[0])\n",
    "            # step['output'] = layer[2]\n",
    "\n",
    "        for walk_step in walk_steps:\n",
    "            if hasattr(walk_step['module'][0], 'nn') and walk_step['module'][0].nn is not None:\n",
    "                # We don't allow any outside nn during message flow process in GINs\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "            elif hasattr(walk_step['module'][0], 'lin') and walk_step['module'][0].lin is not None:\n",
    "                walk_step['module'] = [walk_step['module'][0]]\n",
    "\n",
    "        # print('Walk steps', [walk_steps[i]['module'] for i in range(len(walk_steps))])\n",
    "        # print('fc steps', [fc_steps[i]['module'] for i in range(len(fc_steps))])\n",
    "\n",
    "        return walk_steps, fc_steps\n",
    "\n",
    "    def walks_pick(self,\n",
    "                   edge_index: Tensor,\n",
    "                   pick_edge_indices: List,\n",
    "                   walk_indices: List=[],\n",
    "                   num_layers=0\n",
    "                   ):\n",
    "        walk_indices_list = []\n",
    "        for edge_idx in pick_edge_indices:\n",
    "\n",
    "            # Adding one edge\n",
    "            walk_indices.append(edge_idx)\n",
    "            _, new_src = src, tgt = edge_index[:, edge_idx]\n",
    "            next_edge_indices = np.array((edge_index[0, :] == new_src).nonzero().view(-1))\n",
    "\n",
    "            # Finding next edge\n",
    "            if len(walk_indices) >= num_layers:\n",
    "                # return one walk\n",
    "                walk_indices_list.append(walk_indices.copy())\n",
    "            else:\n",
    "                walk_indices_list += self.walks_pick(edge_index, next_edge_indices, walk_indices, num_layers)\n",
    "\n",
    "            # remove the last edge\n",
    "            walk_indices.pop(-1)\n",
    "\n",
    "        return walk_indices_list\n",
    "\n",
    "class CAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, activation = None):\n",
    "        '''\n",
    "        .. note::\n",
    "            From Pope et al., CAM requires that the layer immediately before the softmax layer be\n",
    "            a global average pooling layer, or in the case of node classification, a graph convolutional\n",
    "            layer. Therefore, for this algorithm to theoretically work, there can be no fully-connected\n",
    "            layers after global pooling. There is no restriction in the code for this, but be warned. \n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            activation (method, optional): activation funciton for final layer in network. If `activation = None`,\n",
    "                explainer assumes linear activation. Use `activation = None` if the activation is applied\n",
    "                within the `forward` method of `model`, only set this parameter if another activation is\n",
    "                applied in the training procedure outside of model. (:default: :obj:`None`)\n",
    "        '''\n",
    "        super().__init__(model=model)\n",
    "        self.model = model\n",
    "\n",
    "        # Set activation function\n",
    "        self.activation = lambda x: x  if activation is None else activation\n",
    "        # i.e. linear activation if none provided\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "                x: torch.Tensor, \n",
    "                node_idx: int, \n",
    "                edge_index: torch.Tensor, \n",
    "                label: int = None,  \n",
    "                y = None,\n",
    "                forward_kwargs: dict = {},\n",
    "                directed: bool = False\n",
    "            ) -> Explanation:\n",
    "        '''\n",
    "        Explain one node prediction by the model\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of node features from the entire graph\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.tensor): edge_index of entire graph\n",
    "            label (int, optional): Label on which to compute the explanation for\n",
    "                this node. If `None`, the predicted label from the model will be\n",
    "                used. (default: :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "            directed (bool, optional): If True, graph is directed.\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        if not directed:\n",
    "            edge_index = to_undirected(edge_index)\n",
    "\n",
    "        if label is None:\n",
    "            if y is None:\n",
    "                label = int(self.__forward_pass(x, edge_index, forward_kwargs).argmax(dim=1).item())\n",
    "            else:\n",
    "                label = y[node_idx]\n",
    "\n",
    "        # Perform walk:\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=False, split_fc=False, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        # Get subgraph:\n",
    "        khop_info = k_hop_subgraph(node_idx = node_idx, num_hops = self.L, edge_index = edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        #cam = torch.zeros(N) # Compute CAM only over the subgraph (all others are zero)\n",
    "        cam = torch.zeros(subgraph_N)\n",
    "        for i in range(subgraph_N):\n",
    "            n = subgraph_nodes[i]\n",
    "            cam[i] += self.__exp_node(n, walk_steps, label)\n",
    "\n",
    "        # Set Explanation class:\n",
    "        exp = Explanation(\n",
    "            node_imp = cam,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def __forward_pass(self, x, edge_index, forward_kwargs = {}):\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, predicted_c):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after last convolutiuonal layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        last_conv_layer = walk_steps[-1]\n",
    "\n",
    "        if isinstance(last_conv_layer['module'][0], GINConv):\n",
    "            weight_vec = last_conv_layer['module'][0].nn.weight[predicted_c, :].detach()  # last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], GCNConv):\n",
    "            weight_vec = last_conv_layer['module'][0].lin.weight[predicted_c, :].detach()\n",
    "        elif isinstance(last_conv_layer['module'][0], torch.nn.Linear):\n",
    "            weight_vec = last_conv_layer['module'][0].weight[predicted_c, :].detach()\n",
    "\n",
    "        F_l_n = F.relu(last_conv_layer['input'][node_idx,:]).detach()\n",
    "\n",
    "        L_cam_n = F.relu(torch.matmul(weight_vec, F_l_n))\n",
    "\n",
    "        return L_cam_n.item()\n",
    "\n",
    "\n",
    "class GradCAM(_BaseDecomposition):\n",
    "    '''\n",
    "    Gradient Class-Activation Mapping for GNNs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: torch.Tensor, criterion = F.cross_entropy):\n",
    "        '''\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            criterion (PyTorch Loss Function): loss function used to train the model.\n",
    "                Needed to pass gradients backwards in the network to obtain gradients.\n",
    "        '''\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "            x: torch.Tensor, \n",
    "            y: torch.Tensor, \n",
    "            node_idx: int, \n",
    "            edge_index: torch.Tensor, \n",
    "            label: int = None, \n",
    "            forward_kwargs: dict = {}, \n",
    "            average_variant: bool = True, \n",
    "            layer: int = 0\n",
    "        ) -> Explanation:\n",
    "        '''\n",
    "        Explain a node in the given graph\n",
    "        Args:\n",
    "            x (torch.Tensor, (n,)): Tensor of node features from the entire graph, with n nodes.\n",
    "            y (torch.Tensor, (n,)): Ground-truth labels for all n nodes in the graph.\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            edge_index (torch.Tensor): Edge_index of entire graph.\n",
    "            label (int, optional): Label for which to compute Grad-CAM against. If None, computes\n",
    "                the Grad-CAM with respect to the model's predicted class for this node.\n",
    "                (default :obj:`None`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. (default: :obj:`None`)\n",
    "            average_variant (bool, optional): If True, computes the average Grad-CAM across all convolutional\n",
    "                layers in the model. If False, computes Grad-CAM for `layer`. (default: :obj:`True`)\n",
    "            layer (int, optional): Layer by which to compute the Grad-CAM. Argument only has an effect if \n",
    "                `average_variant == True`. Must be less-than the total number of convolutional layers\n",
    "                in the model. (default: :obj:`0`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        x = x.detach().clone()\n",
    "        y = y.detach().clone()\n",
    "\n",
    "        x.requires_grad = True\n",
    "\n",
    "        if label is None:\n",
    "            pred = self.__forward_pass(x, y, edge_index, forward_kwargs)[0][node_idx, :].reshape(1, -1)\n",
    "            y[node_idx] = pred.argmax(dim=1).item()\n",
    "        else: # Transform node_idx's label if provided by user\n",
    "            pred, loss = self.__forward_pass(x, y, edge_index, forward_kwargs)\n",
    "            y[node_idx] = label\n",
    "\n",
    "        walk_steps, _ = self.extract_step(x, edge_index, detach=True, split_fc=True, forward_kwargs = forward_kwargs)\n",
    "\n",
    "        khop_info = k_hop_subgraph(node_idx, self.L, edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        N = maybe_num_nodes(edge_index, None)\n",
    "        subgraph_N = len(subgraph_nodes.tolist())\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "\n",
    "        if average_variant:\n",
    "            # Size of all nodes in the subgraph:\n",
    "            avg_gcam = torch.zeros(subgraph_N)\n",
    "\n",
    "            for l in range(self.L):\n",
    "                # Compute gradients for this layer ahead of time:\n",
    "                gradients = self.__grad_by_layer(l)\n",
    "\n",
    "                for i in range(subgraph_N): # Over all subgraph nodes\n",
    "                    n = subgraph_nodes[i]\n",
    "                    avg_gcam[i] += self.__get_gCAM_layer(walk_steps, l, n, gradients)\n",
    "\n",
    "            avg_gcam /= self.L # Apply average\n",
    "\n",
    "            exp.node_imp = avg_gcam\n",
    "\n",
    "        else:\n",
    "            assert layer < len(walk_steps), \"Layer must be an index of convolutional layers\"\n",
    "\n",
    "            gcam = torch.zeros(subgraph_N)\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "            for i in range(subgraph_N):\n",
    "                n = subgraph_nodes[i]\n",
    "                gcam[i] += self.__get_gCAM_layer(walk_steps, layer, n, gradients)#[0]\n",
    "\n",
    "            exp.node_imp = gcam\n",
    "\n",
    "        return exp\n",
    "        \n",
    "    def __forward_pass(self, x, label, edge_index, forward_kwargs):\n",
    "        x.requires_grad = True # Enforce that x needs gradient\n",
    "\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        loss = self.criterion(pred, label)\n",
    "        loss.backward() # Propagate loss backward through network\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def __grad_by_layer(self, layer):\n",
    "        # Index 0 of parameters to avoid calculating gradients for biases\n",
    "        module_at_layer = list(self.model.children())[layer]\n",
    "\n",
    "        if isinstance(module_at_layer, GCNConv):\n",
    "            grad = module_at_layer.lin.weight.grad\n",
    "        elif isinstance(module_at_layer, GINConv):\n",
    "            grad = module_at_layer.nn.weight.grad\n",
    "        else:\n",
    "            grad = module_at_layer.weight.grad\n",
    "\n",
    "        return grad.mean(dim=1)\n",
    "\n",
    "    def __get_gCAM_layer(self, walk_steps, layer, node_idx = None, gradients = None):\n",
    "        # Gets Grad CAM for one layer\n",
    "        if gradients is None:\n",
    "            # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "            # ''        '' shape: [k,] - k= # output features from layer l\n",
    "            gradients = self.__grad_by_layer(layer)\n",
    "\n",
    "        if node_idx is None: # Need to compute for entire graph:\n",
    "            node_explanations = []\n",
    "            for n in range(self.N):\n",
    "                node_explanations.append(self.__exp_node(n, walk_steps, layer, gradients))\n",
    "\n",
    "            return node_explanations\n",
    "\n",
    "        # Return for only one node:\n",
    "        return self.__exp_node(node_idx, walk_steps, layer, gradients)\n",
    "\n",
    "    def __exp_node(self, node_idx, walk_steps, layer, gradients):\n",
    "        '''\n",
    "        Gets explanation for one node\n",
    "        Assumes ReLU activation after each convolutional layer\n",
    "        TODO: Fix activation function assumption\n",
    "        '''\n",
    "        # \\alpha^{l,c} = Average over nodes of gradients for layer l, after activation over c\n",
    "        # ''        '' shape: [k,] - k= # input features to layer l\n",
    "\n",
    "        # Activations for node n\n",
    "        F_l_n = F.relu(walk_steps[layer]['output'][node_idx,:]).detach()\n",
    "        L_cam_n = F.relu(torch.matmul(gradients, F_l_n)) # Combine gradients and activations\n",
    "\n",
    "        return L_cam_n.item()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from graphxai.utils import Explanation\n",
    "# from ._decomp_base_old import _BaseDecomposition\n",
    "\n",
    "def clip_hook(grad):\n",
    "    # Apply ReLU activation to gradient\n",
    "    return torch.clamp(grad, min=0)\n",
    "import gc\n",
    "\n",
    "class GuidedBP(_BaseDecomposition):\n",
    "\n",
    "    def __init__(self, model, criterion = F.cross_entropy, enforce_requires_grad = True):\n",
    "        '''\n",
    "        Args:\n",
    "            model (torch.nn.Module): model on which to make predictions\n",
    "            criterion (PyTorch Loss Function): loss function used to train the model.\n",
    "                Needed to pass gradients backwards in the network to obtain gradients.\n",
    "        '''\n",
    "        super().__init__(model)\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.L = len([module for module in self.model.modules() if isinstance(module, MessagePassing)])\n",
    "\n",
    "        self.registered_hooks = []\n",
    "\n",
    "        self.enforce_requires_grad = enforce_requires_grad\n",
    "\n",
    "    def get_explanation_node(self, \n",
    "                x: torch.Tensor, \n",
    "                y: torch.Tensor,\n",
    "                edge_index: torch.Tensor,  \n",
    "                node_idx: int, \n",
    "                aggregate_node_imp = torch.sum,\n",
    "                forward_kwargs: dict = {}\n",
    "            ) -> Explanation:\n",
    "        '''\n",
    "        Get Guided Backpropagation explanation for one node in the graph\n",
    "        Args:\n",
    "            x (torch.tensor): tensor of node features from the entire graph\n",
    "            y (torch.Tensor): Ground truth labels correspond to each node's \n",
    "                classification. This argument is input to the `criterion` \n",
    "                function provided in `__init__()`.\n",
    "            edge_index (torch.tensor): Edge_index of entire graph.\n",
    "            node_idx (int): node index for which to explain a prediction around\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the enclosing \n",
    "                subgraph. Must support `dim` argument.\n",
    "                (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)\n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method.\n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [nodes_in_khop, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `enc_subgraph`: :obj:`graphxai.utils.EnclosingSubgraph`\n",
    "        '''\n",
    "\n",
    "        # Run whole-graph prediction:\n",
    "        if self.enforce_requires_grad:\n",
    "            try:\n",
    "                x = x.detach().clone()\n",
    "                x.requires_grad = True\n",
    "            except:\n",
    "                pass\n",
    "        assert x.requires_grad, 'x must have requires_grad == True'\n",
    "\n",
    "        # Perform the guided backprop:\n",
    "        xhook = x.register_hook(clip_hook)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        pred = self.__forward_pass(x, edge_index, forward_kwargs)\n",
    "        loss = self.criterion(pred, y)\n",
    "        self.__apply_hooks()\n",
    "        loss.backward()\n",
    "        self.__rm_hooks()\n",
    "\n",
    "        xhook.remove() # Remove hook from x\n",
    "\n",
    "        graph_exp = x.grad\n",
    "\n",
    "        khop_info = k_hop_subgraph(node_idx = node_idx, num_hops = self.L, edge_index = edge_index)\n",
    "        subgraph_nodes = khop_info[0]\n",
    "\n",
    "        node_imp = aggregate_node_imp(torch.stack([graph_exp[i,:] for i in subgraph_nodes]).detach(), dim=1)\n",
    "\n",
    "        # Get only those explanations for nodes in the subgraph:\n",
    "        exp = Explanation(\n",
    "            node_imp = node_imp,\n",
    "            node_idx = node_idx\n",
    "        )\n",
    "        \n",
    "        exp.set_enclosing_subgraph(khop_info)\n",
    "        return exp\n",
    "\n",
    "    def get_explanation_graph(self, \n",
    "                x: torch.Tensor, \n",
    "                y: torch.Tensor, \n",
    "                edge_index: torch.Tensor, \n",
    "                aggregate_node_imp = torch.sum,\n",
    "                forward_kwargs: dict = {}\n",
    "        ) -> Explanation:\n",
    "        '''\n",
    "        Explain a whole-graph prediction with Guided Backpropagation\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): Tensor of node features from the entire graph.\n",
    "            y (torch.tensor): Ground truth label of given input. This argument is \n",
    "                input to the `criterion` function provided in `__init__()`.\n",
    "            edge_index (torch.tensor): Edge_index of entire graph.\n",
    "            aggregate_node_imp (function, optional): torch function that aggregates\n",
    "                all node importance feature-wise scores across the graph. \n",
    "                Must support `dim` argument. (:default: :obj:`torch.sum`)\n",
    "            forward_kwargs (dict, optional): Additional arguments to model.forward \n",
    "                beyond x and edge_index. Must be keyed on argument name. \n",
    "                (default: :obj:`{}`)   \n",
    "\n",
    "        :rtype: :class:`graphxai.Explanation`\n",
    "\n",
    "        Returns:\n",
    "            exp (:class:`Explanation`): Explanation output from the method. \n",
    "                Fields are:\n",
    "                `feature_imp`: :obj:`None`\n",
    "                `node_imp`: :obj:`torch.Tensor, [num_nodes, features]`\n",
    "                `edge_imp`: :obj:`None`\n",
    "                `graph`: :obj:`torch_geometric.data.Data`\n",
    "        '''\n",
    "\n",
    "        # Run whole-graph prediction:\n",
    "        try:\n",
    "            x.requires_grad = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        assert x.requires_grad, 'x must have requires_grad == True' \n",
    "\n",
    "        # Perform the guided backprop:\n",
    "        xhook = x.register_hook(clip_hook)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        pred = self.__forward_pass(x, edge_index, forward_kwargs)\n",
    "        loss = self.criterion(pred, y)\n",
    "        self.__apply_hooks()\n",
    "        loss.backward()\n",
    "        self.__rm_hooks()\n",
    "\n",
    "        xhook.remove() # Remove hook from x\n",
    "\n",
    "        node_imp = aggregate_node_imp(x.grad, dim=1)\n",
    "\n",
    "        exp = Explanation(\n",
    "            node_imp = node_imp\n",
    "        )\n",
    "    \n",
    "        exp.set_whole_graph(Data(x, edge_index))\n",
    "\n",
    "        return exp\n",
    "\n",
    "    def __apply_hooks(self):\n",
    "        self.registered_hooks = []\n",
    "        for p in self.model.parameters():\n",
    "            h = p.register_hook(clip_hook)\n",
    "            self.registered_hooks.append(h)\n",
    "\n",
    "    def __rm_hooks(self):\n",
    "        for h in self.registered_hooks:\n",
    "            h.remove()\n",
    "        self.registered_hooks = []\n",
    "    \n",
    "    def __forward_pass(self, x, edge_index, forward_kwargs):\n",
    "        # Forward pass:\n",
    "        self.model.eval()\n",
    "        self.__apply_hooks()\n",
    "        pred = self.model(x, edge_index, **forward_kwargs)\n",
    "\n",
    "        return pred\n",
    "from graphxai.utils import Explanation\n",
    "def IG(node_idx: int, \n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor, \n",
    "        y: Optional[torch.Tensor] = None,\n",
    "        num_hops: Optional[int] = None, \n",
    "        steps: Optional[int] = 40,\n",
    "        model = None,\n",
    "        criterion = None):\n",
    "    \"\"\"\n",
    "    Explain a node prediction.\n",
    "\n",
    "    Args:\n",
    "        node_idx (int): Index of the node to be explained.\n",
    "        edge_index (torch.Tensor, [2 x m]): Edge index of the graph.\n",
    "        x (torch.Tensor, [n x d]): Node features.\n",
    "        label (torch.Tensor, [n x ...]): Labels to explain.\n",
    "        y (torch.Tensor): Same as `label`, provided for general \n",
    "            compatibility in the arguments. (:default: :obj:`None`)\n",
    "        num_hops (int, optional): Number of hops in the enclosing \n",
    "            subgraph. If `None`, set to the number of layers in \n",
    "            the GNN. (:default: :obj:`None`)\n",
    "        steps (int, optional): Number of steps for the Riemannian \n",
    "            integration. (:default: :obj:`40`)\n",
    "\n",
    "    Returns:\n",
    "        exp (:class:`Explanation`): Explanation output from the method.\n",
    "            Fields are:\n",
    "            `feature_imp`: :obj:`torch.Tensor, [x.shape[1],]`\n",
    "            `node_imp`: :obj:`torch.Tensor, [nodes_in_khop,]`\n",
    "            `edge_imp`: :obj:`None`\n",
    "            `enc_subgraph`: :class:`graphxai.utils.EnclosingSubgraph`\n",
    "    \"\"\"\n",
    "\n",
    "    if (y is None):\n",
    "        raise ValueError('Either label or y should be provided for Integrated Gradients')\n",
    "\n",
    "    label = y[node_idx]\n",
    "    if len(label.shape) == 0:\n",
    "        label = label.unsqueeze(dim=0)\n",
    "\n",
    "    num_hops = num_hops if num_hops is not None else 1\n",
    "    khop_info = subset, sub_edge_index, mapping, _ = \\\n",
    "        k_hop_subgraph(node_idx, num_hops, edge_index,\n",
    "                        relabel_nodes=True, num_nodes=x.shape[0])\n",
    "    sub_x = x[subset]\n",
    "\n",
    "    model.eval()\n",
    "    grads = torch.zeros(steps+1, x.shape[1]).to(device)\n",
    "\n",
    "    # Perform Riemannian integration\n",
    "    for i in range(steps+1):\n",
    "        with torch.no_grad():\n",
    "            baseline = torch.zeros_like(sub_x).to(device)  # TODO: baseline all 0s, all 1s, ...?\n",
    "            temp_x = baseline + (float(i)/steps) * (sub_x.clone()-baseline)\n",
    "        temp_x.requires_grad = True\n",
    "        output = model(temp_x, sub_edge_index)\n",
    "        loss = criterion(output[mapping], label)\n",
    "        loss.backward()\n",
    "        grad = temp_x.grad[torch.where(subset==node_idx)[0].item()]\n",
    "        grads[i] = grad\n",
    "\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = torch.mean(grads, axis=0)\n",
    "\n",
    "    # Integrated gradients for only node_idx:\n",
    "    # baseline[0] just gets a single-value 0-tensor\n",
    "    integrated_gradients = ((x[torch.where(subset == node_idx)[0].item()]\n",
    "                                - baseline[0]) * avg_grads)\n",
    "\n",
    "    # Integrated gradients across the enclosing subgraph:\n",
    "    all_node_ig = ((x[subset] - baseline) * avg_grads)\n",
    "    node_importances = torch.sum(all_node_ig, dim=1)\n",
    "    exp = Explanation(\n",
    "        feature_imp = integrated_gradients,\n",
    "        node_imp = torch.sum(all_node_ig, dim=1),\n",
    "        node_idx = node_idx\n",
    "    )\n",
    "    exp.set_enclosing_subgraph(khop_info)\n",
    "    gc.collect()\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sufficiency Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, sys\n",
    "from typing import Union, List\n",
    "\n",
    "from networkx.classes.function import to_undirected\n",
    "import networkx as nx\n",
    "import ipdb\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from numpy import ndarray\n",
    "from torch_geometric.utils import to_networkx\n",
    "import gc\n",
    "\n",
    "from graphxai.explainers import CAM, GradCAM, GNNExplainer\n",
    "# from graphxai.explainers.utils.visualizations import visualize_subgraph_explanation\n",
    "from graphxai.visualization.visualizations import visualize_subgraph_explanation\n",
    "from graphxai.visualization.explanation_vis import visualize_node_explanation\n",
    "from graphxai.gnn_models.node_classification import GCN, train, test\n",
    "from graphxai.gnn_models.node_classification.testing import GCN_3layer_basic, GIN_3layer_basic\n",
    "\n",
    "from graphxai.gnn_models.node_classification import GCN, train, test\n",
    "from graphxai.gnn_models.node_classification.testing import GCN_3layer_basic, train, test\n",
    "\n",
    "from graphxai.datasets.shape_graph import ShapeGGen\n",
    "from graphxai.utils import to_networkx_conv, Explanation, distance\n",
    "from graphxai.utils.perturb import rewire_edges\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tqdm import tqdm\n",
    "\n",
    "B = {1,5,10,20,50}\n",
    "len_b = len(B)\n",
    "def sufficiency(generated_exp: list, shape_graph: any, model: torch.nn.Module) -> list:\n",
    "    '''\n",
    "    Args:\n",
    "        gt_exp (list): Ground truth explanation from the dataset.\n",
    "        generated_exp (Explanation): Explanation output by an explainer.\n",
    "    '''\n",
    "\n",
    "    # TODO: 1) Implement perturbations for continuous and discrete node attribute features\n",
    "    suff = list() # GEF = 1/m * sum(||F(x)-F(y)||) where F(x) is the original importances and F(y) is the perturbed\n",
    "\n",
    "    # Accessing the enclosing subgraph. Will be the same for both explanation.:\n",
    "    with tqdm(total=len(generated_exp)) as pbar:\n",
    "        for exp in generated_exp:\n",
    "            pbar.update(1)\n",
    "            exp_subgraph = exp.enc_subgraph\n",
    "            dist_total = 0\n",
    "            for q in B:\n",
    "                top_k = q / 100\n",
    "                # Identifying the top_k nodes in the explanation subgraph\n",
    "                top_k_nodes = exp.node_imp.topk(int(exp.node_imp.shape[0] * top_k))[1]\n",
    "                rem_nodes = []\n",
    "                for node in range(exp.node_imp.shape[0]):\n",
    "                    if node not in top_k_nodes:\n",
    "                        rem_nodes.append([k for k, v in exp.node_reference.items() if v == node][0])\n",
    "\n",
    "                # Getting the softmax vector for the original graph\n",
    "                org_vec = model(shape_graph.x, shape_graph.edge_index)[exp.node_idx]\n",
    "                org_softmax = F.softmax(org_vec, dim=-1)\n",
    "                # Getting the softmax vector for the perturbed graph\n",
    "                pert_x = shape_graph.x.clone()\n",
    "\n",
    "                # Removing the unimportant nodes by masking\n",
    "                pert_x[rem_nodes] = torch.zeros_like(pert_x[rem_nodes])  # torch.normal(0, 0.1, pert_x[rem_nodes].shape)\n",
    "                pert_vec = model(pert_x, shape_graph.edge_index)[exp.node_idx]\n",
    "\n",
    "                # check if the graph is disconnected!!\n",
    "                pert_softmax = F.softmax(pert_vec, dim=-1)\n",
    "\n",
    "                # summing the differences between the two explanations\n",
    "                dist = euclidean(org_softmax.detach().cpu().numpy(), pert_softmax.detach().cpu().numpy())\n",
    "\n",
    "                dist_total += dist\n",
    "                gc.collect()\n",
    "            suff.append(dist_total / len_b)\n",
    "    return suff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbp_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data, explainability_model)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    bp_exps = list()\n",
    "    model.eval()\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = explainability_model.get_explanation_node(node_idx=node, x=data_object.x, edge_index=data_object.edge_index, y=data_object.y)\n",
    "            bp_exps.append(exp)\n",
    "    return bp_exps\n",
    "\n",
    "def cam_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data, explainability_model)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    cam_exps = list()\n",
    "    model.eval()\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = explainability_model.get_explanation_node(node_idx=node, x = data_object.x, edge_index = data_object.edge_index,  y = data_object.y)\n",
    "            cam_exps.append(exp)\n",
    "    return cam_exps\n",
    "\n",
    "def ig_exps(model: torch.nn.Module, graph, data_object: torch_geometric.data.Data)->list:\n",
    "    \"\"\"For all nodes, we get the explanations from the current model, returned as list\"\"\"\n",
    "    ig_exps = list()\n",
    "    model.eval()\n",
    "    with tqdm(total=len(graph.nodes())) as pbar:\n",
    "\n",
    "        for node in graph.nodes():\n",
    "            pbar.update(1)\n",
    "            exp = IG(node_idx=node, x=data_object.x, edge_index=data_object.edge_index, y=data_object.y, model=model, num_hops=1, criterion=torch.nn.CrossEntropyLoss() )\n",
    "            ig_exps.append(exp)\n",
    "    return ig_exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the train and test data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_mask = torch.zeros((data.x.shape[0]), dtype=torch.long); test_mask = torch.zeros((data.x.shape[0]), dtype=torch.long)\n",
    "\n",
    "train_indices = torch.randint(0, data.x.shape[0], (1, int(data.x.shape[0]*0.8)))\n",
    "test_indices = torch.randint(0, data.x.shape[0], (1, int(data.x.shape[0]*0.2)))\n",
    "\n",
    "train_mask[train_indices] = 1\n",
    "test_mask[test_indices] = 1\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.test_mask].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.80923080444336\n",
      "60.321537017822266\n",
      "59.754486083984375\n",
      "51.11375427246094\n",
      "47.290191650390625\n",
      "44.62841033935547\n",
      "58.99668502807617\n",
      "42.52287673950195\n",
      "42.19537353515625\n",
      "47.56452941894531\n",
      "44.36557388305664\n",
      "42.75148391723633\n",
      "41.104278564453125\n",
      "41.55619812011719\n",
      "43.86784744262695\n",
      "41.529483795166016\n",
      "40.30516052246094\n",
      "41.91328430175781\n",
      "40.45734786987305\n",
      "39.90081787109375\n",
      "40.314998626708984\n",
      "40.66602325439453\n",
      "40.37571716308594\n",
      "39.71137237548828\n",
      "38.830345153808594\n",
      "39.82859420776367\n",
      "39.24330139160156\n",
      "39.528602600097656\n",
      "39.89796447753906\n",
      "39.53315734863281\n",
      "43.11159133911133\n",
      "44.9801139831543\n",
      "39.44657516479492\n",
      "39.521644592285156\n",
      "39.90966033935547\n",
      "39.120033264160156\n",
      "39.66096115112305\n",
      "42.01803970336914\n",
      "38.690330505371094\n",
      "41.13526153564453\n",
      "39.742027282714844\n",
      "40.70431137084961\n",
      "39.01545715332031\n",
      "38.46323776245117\n",
      "38.56859588623047\n",
      "38.914241790771484\n",
      "39.22187805175781\n",
      "39.3608512878418\n",
      "38.44103240966797\n",
      "39.50712203979492\n",
      "39.3946418762207\n",
      "38.84636306762695\n",
      "39.67498016357422\n",
      "38.06282424926758\n",
      "38.588748931884766\n",
      "38.76044464111328\n",
      "39.44347381591797\n",
      "38.0645637512207\n",
      "38.91351318359375\n",
      "39.59695816040039\n",
      "39.07111740112305\n",
      "38.13896179199219\n",
      "37.99758529663086\n",
      "38.97056579589844\n",
      "38.168373107910156\n",
      "38.59956741333008\n",
      "38.7757453918457\n",
      "38.95701599121094\n",
      "38.09574890136719\n",
      "38.470542907714844\n",
      "43.15769577026367\n",
      "37.97787094116211\n",
      "38.40180587768555\n",
      "38.500858306884766\n",
      "39.02592468261719\n",
      "41.02655792236328\n",
      "38.39847183227539\n",
      "39.41852569580078\n",
      "41.546146392822266\n",
      "39.524532318115234\n",
      "41.1462516784668\n",
      "39.98871994018555\n",
      "38.844181060791016\n",
      "38.872562408447266\n",
      "44.938873291015625\n",
      "39.14277267456055\n",
      "40.59564208984375\n",
      "39.75751876831055\n",
      "39.784976959228516\n",
      "39.65264129638672\n",
      "39.41796112060547\n",
      "39.9005241394043\n",
      "39.33245849609375\n",
      "39.1926383972168\n",
      "38.749481201171875\n",
      "38.30802536010742\n",
      "38.12821960449219\n",
      "38.067237854003906\n",
      "38.255855560302734\n",
      "38.195552825927734\n",
      "37.95207595825195\n",
      "54.03569793701172\n",
      "39.120277404785156\n",
      "38.70217514038086\n",
      "38.26386260986328\n",
      "38.48506546020508\n",
      "38.19949722290039\n",
      "38.34172821044922\n",
      "38.233829498291016\n",
      "38.42992401123047\n",
      "38.27176284790039\n",
      "38.30788803100586\n",
      "39.234336853027344\n",
      "38.409427642822266\n",
      "38.7372932434082\n",
      "38.05203628540039\n",
      "40.79975509643555\n",
      "38.0079231262207\n",
      "38.000728607177734\n",
      "38.70925521850586\n",
      "37.753173828125\n",
      "39.955650329589844\n",
      "44.30223083496094\n",
      "38.46916198730469\n",
      "40.385986328125\n",
      "40.77707290649414\n",
      "39.12964630126953\n",
      "40.103023529052734\n",
      "38.217037200927734\n",
      "39.1253547668457\n",
      "39.855770111083984\n",
      "39.36538314819336\n",
      "38.9425048828125\n",
      "39.772403717041016\n",
      "38.563167572021484\n",
      "38.84767532348633\n",
      "38.41712951660156\n",
      "38.3655891418457\n",
      "38.318321228027344\n",
      "39.77010726928711\n",
      "42.388824462890625\n",
      "38.18684005737305\n",
      "38.453147888183594\n",
      "38.365848541259766\n",
      "38.23957443237305\n",
      "38.29929733276367\n",
      "41.82099151611328\n",
      "38.616294860839844\n",
      "37.94098663330078\n",
      "38.212894439697266\n",
      "38.070899963378906\n",
      "38.2104377746582\n",
      "38.24978256225586\n",
      "38.690006256103516\n",
      "38.29608917236328\n",
      "38.60374069213867\n",
      "38.03704071044922\n",
      "38.77375030517578\n",
      "37.45858383178711\n",
      "38.06123352050781\n",
      "38.086669921875\n",
      "37.87835693359375\n",
      "38.477516174316406\n",
      "37.68120193481445\n",
      "40.230133056640625\n",
      "37.9849967956543\n",
      "38.29125213623047\n",
      "37.66300964355469\n",
      "38.78334426879883\n",
      "37.73371887207031\n",
      "37.89564514160156\n",
      "38.093421936035156\n",
      "37.65293502807617\n",
      "38.053470611572266\n",
      "37.81432342529297\n",
      "38.162940979003906\n",
      "38.019004821777344\n",
      "39.012569427490234\n",
      "37.85238265991211\n",
      "37.65703582763672\n",
      "37.845115661621094\n",
      "37.78850555419922\n",
      "37.74127197265625\n",
      "38.39715576171875\n",
      "38.046485900878906\n",
      "37.70489501953125\n",
      "38.33677673339844\n",
      "38.0404167175293\n",
      "38.172767639160156\n",
      "38.12551498413086\n",
      "37.89413833618164\n",
      "37.78966522216797\n",
      "38.28903579711914\n",
      "37.88159942626953\n",
      "37.72505569458008\n",
      "37.709632873535156\n",
      "37.78517150878906\n",
      "37.79014587402344\n",
      "37.76420211791992\n",
      "37.77141571044922\n",
      "39.188514709472656\n",
      "38.293636322021484\n",
      "37.89714050292969\n",
      "37.70212936401367\n",
      "37.644264221191406\n",
      "38.07558822631836\n",
      "37.81328201293945\n",
      "37.688568115234375\n",
      "38.02593231201172\n",
      "37.609535217285156\n",
      "37.71910858154297\n",
      "37.723846435546875\n",
      "37.598506927490234\n",
      "37.90032196044922\n",
      "38.148921966552734\n",
      "37.791778564453125\n",
      "37.94366455078125\n",
      "37.835182189941406\n",
      "37.59553527832031\n",
      "37.588138580322266\n",
      "37.9018440246582\n",
      "38.70881652832031\n",
      "37.73503494262695\n",
      "37.56496047973633\n",
      "37.618629455566406\n",
      "37.74267578125\n",
      "37.585792541503906\n",
      "38.085121154785156\n",
      "37.5595588684082\n",
      "37.72991180419922\n",
      "37.74883270263672\n",
      "37.85914993286133\n",
      "38.08836364746094\n",
      "37.68059539794922\n",
      "37.943626403808594\n",
      "37.827945709228516\n",
      "38.2113037109375\n",
      "37.536441802978516\n",
      "37.861446380615234\n",
      "38.3797492980957\n",
      "38.37579345703125\n",
      "42.19524002075195\n",
      "37.831825256347656\n",
      "37.90583419799805\n",
      "38.132598876953125\n",
      "37.920955657958984\n",
      "37.974491119384766\n",
      "38.29942321777344\n",
      "37.82222366333008\n",
      "37.70888900756836\n",
      "38.125587463378906\n",
      "37.771915435791016\n",
      "37.659149169921875\n",
      "37.88039016723633\n",
      "37.80400466918945\n",
      "37.65926742553711\n",
      "37.77313232421875\n",
      "37.75053405761719\n",
      "37.62795639038086\n",
      "38.307621002197266\n",
      "38.09096145629883\n",
      "37.812801361083984\n",
      "37.52971649169922\n",
      "37.5476188659668\n",
      "37.94697570800781\n",
      "38.34156799316406\n",
      "37.657772064208984\n",
      "37.75311279296875\n",
      "37.89038848876953\n",
      "37.68185806274414\n",
      "37.58979797363281\n",
      "37.744720458984375\n",
      "37.33234786987305\n",
      "37.44033432006836\n",
      "37.49945831298828\n",
      "37.57986068725586\n",
      "37.44609451293945\n",
      "37.83271408081055\n",
      "37.96477508544922\n",
      "39.09929656982422\n",
      "37.47716522216797\n",
      "37.509517669677734\n",
      "37.64744186401367\n",
      "37.81467819213867\n",
      "37.81832504272461\n",
      "38.00227355957031\n",
      "37.73686599731445\n",
      "38.175201416015625\n",
      "38.293922424316406\n",
      "37.72473907470703\n",
      "37.82779312133789\n",
      "37.82202911376953\n",
      "38.12854766845703\n",
      "37.62227249145508\n",
      "37.62403869628906\n",
      "39.317474365234375\n",
      "37.88133239746094\n",
      "37.82602310180664\n",
      "37.827537536621094\n",
      "38.5521240234375\n",
      "37.64917755126953\n",
      "37.49264144897461\n",
      "37.752166748046875\n",
      "37.896244049072266\n",
      "37.69298553466797\n",
      "37.819313049316406\n",
      "37.703697204589844\n",
      "37.75548553466797\n",
      "37.573814392089844\n",
      "37.65238571166992\n",
      "37.661746978759766\n",
      "37.768714904785156\n",
      "37.863243103027344\n",
      "37.607032775878906\n",
      "37.76902389526367\n",
      "37.783260345458984\n",
      "37.82107925415039\n",
      "37.53700637817383\n",
      "37.55670928955078\n",
      "37.78314208984375\n",
      "37.598915100097656\n",
      "37.773799896240234\n",
      "38.0468864440918\n",
      "37.450523376464844\n",
      "37.491214752197266\n",
      "37.523189544677734\n",
      "37.800960540771484\n",
      "37.534820556640625\n",
      "38.2066764831543\n",
      "37.690879821777344\n",
      "37.71194839477539\n",
      "37.614437103271484\n",
      "37.750953674316406\n",
      "37.75733184814453\n",
      "37.66047668457031\n",
      "37.69683074951172\n",
      "37.46978759765625\n",
      "37.537330627441406\n",
      "37.9456787109375\n",
      "37.766868591308594\n",
      "37.79167938232422\n",
      "37.56739807128906\n",
      "37.458030700683594\n",
      "37.85224151611328\n",
      "37.71299743652344\n",
      "37.662513732910156\n",
      "37.84897232055664\n",
      "37.708316802978516\n",
      "37.71918487548828\n",
      "37.45917510986328\n",
      "37.92753982543945\n",
      "37.69397735595703\n",
      "37.533390045166016\n",
      "37.4782829284668\n",
      "37.53703689575195\n",
      "37.40571975708008\n",
      "38.51594161987305\n",
      "37.58728790283203\n",
      "37.59609603881836\n",
      "37.71891403198242\n",
      "37.54450607299805\n",
      "37.65059280395508\n",
      "37.57792282104492\n",
      "37.648860931396484\n",
      "37.698219299316406\n",
      "37.869869232177734\n",
      "37.575706481933594\n",
      "37.63445281982422\n",
      "38.430572509765625\n",
      "37.380516052246094\n",
      "37.585269927978516\n",
      "37.686771392822266\n",
      "37.69398498535156\n",
      "37.6592903137207\n",
      "37.38166809082031\n",
      "37.648529052734375\n",
      "37.73968505859375\n",
      "37.62836837768555\n",
      "37.37497329711914\n",
      "37.75877380371094\n",
      "37.38435363769531\n",
      "37.697998046875\n",
      "37.4611930847168\n",
      "37.300636291503906\n",
      "37.6671028137207\n",
      "37.47201919555664\n",
      "38.81676483154297\n",
      "37.925048828125\n",
      "37.290130615234375\n",
      "37.459014892578125\n",
      "37.741798400878906\n",
      "37.82208251953125\n",
      "37.58854675292969\n",
      "37.71507263183594\n",
      "37.76701354980469\n",
      "37.448036193847656\n",
      "37.78657150268555\n",
      "37.73456954956055\n",
      "38.055843353271484\n",
      "37.43156433105469\n",
      "37.73871612548828\n",
      "37.803733825683594\n",
      "37.76774597167969\n",
      "37.852420806884766\n",
      "37.44115447998047\n",
      "37.64512252807617\n",
      "37.698699951171875\n",
      "37.75115966796875\n",
      "37.53165054321289\n",
      "37.60674285888672\n",
      "37.71221923828125\n",
      "37.65439224243164\n",
      "38.04631042480469\n",
      "37.57558059692383\n",
      "37.690120697021484\n",
      "37.6231689453125\n",
      "37.23735427856445\n",
      "37.413116455078125\n",
      "37.6065673828125\n",
      "37.56135177612305\n",
      "37.36176300048828\n",
      "37.58115768432617\n",
      "37.354129791259766\n",
      "37.61528778076172\n",
      "37.525428771972656\n",
      "37.76398849487305\n",
      "37.66737365722656\n",
      "37.62874984741211\n",
      "37.44646453857422\n",
      "37.44632339477539\n",
      "38.02215576171875\n",
      "37.413917541503906\n",
      "37.707645416259766\n",
      "37.34969711303711\n",
      "37.26141357421875\n",
      "37.827945709228516\n",
      "37.272560119628906\n",
      "37.623741149902344\n",
      "37.70991897583008\n",
      "37.259910583496094\n",
      "38.052818298339844\n",
      "37.30966567993164\n",
      "38.00186538696289\n",
      "37.751220703125\n",
      "37.138545989990234\n",
      "37.6547966003418\n",
      "37.60725021362305\n",
      "37.55115509033203\n",
      "37.867286682128906\n",
      "37.795082092285156\n",
      "37.64334487915039\n",
      "36.80268859863281\n",
      "38.41694259643555\n",
      "37.447750091552734\n",
      "37.632293701171875\n",
      "37.573081970214844\n",
      "37.51375961303711\n",
      "37.58213806152344\n",
      "37.720947265625\n",
      "37.48350143432617\n",
      "37.72395706176758\n",
      "37.643280029296875\n",
      "37.646060943603516\n",
      "37.242679595947266\n",
      "37.18722915649414\n",
      "37.44749069213867\n",
      "37.40777587890625\n",
      "37.14393997192383\n",
      "37.31245803833008\n",
      "37.02848815917969\n",
      "38.24837112426758\n",
      "37.11618423461914\n",
      "37.22001266479492\n",
      "37.47270965576172\n",
      "37.17675018310547\n",
      "37.402462005615234\n",
      "37.149383544921875\n",
      "37.35908126831055\n",
      "37.52181625366211\n",
      "37.755680084228516\n",
      "37.6553955078125\n",
      "37.20404815673828\n",
      "38.05938720703125\n",
      "37.399261474609375\n",
      "37.765262603759766\n",
      "37.10079574584961\n",
      "37.49773406982422\n",
      "36.70471954345703\n",
      "37.590572357177734\n",
      "37.38652038574219\n",
      "36.92681884765625\n",
      "37.28545379638672\n",
      "37.12430191040039\n",
      "37.355865478515625\n",
      "37.228599548339844\n",
      "37.48396682739258\n",
      "37.57490921020508\n",
      "37.356475830078125\n",
      "36.8658447265625\n",
      "37.87295150756836\n",
      "37.63526916503906\n",
      "37.4879035949707\n",
      "37.44454574584961\n",
      "37.8166389465332\n",
      "36.55952072143555\n",
      "38.15972900390625\n",
      "37.35688400268555\n",
      "37.7186164855957\n",
      "36.642112731933594\n",
      "37.688682556152344\n",
      "36.491641998291016\n",
      "36.38914108276367\n",
      "36.74689865112305\n",
      "36.550758361816406\n",
      "36.4389762878418\n",
      "38.07086944580078\n",
      "37.13758087158203\n",
      "37.07091522216797\n",
      "37.36985397338867\n",
      "37.242706298828125\n",
      "37.38349914550781\n",
      "37.50539016723633\n",
      "36.78921127319336\n",
      "37.30446243286133\n",
      "37.06686782836914\n",
      "36.41501998901367\n",
      "37.60129928588867\n",
      "36.56432342529297\n",
      "37.56466293334961\n",
      "36.49237060546875\n",
      "37.49484634399414\n",
      "36.966697692871094\n",
      "38.868839263916016\n",
      "36.651283264160156\n",
      "36.86417770385742\n",
      "36.572818756103516\n",
      "36.92600631713867\n",
      "36.516963958740234\n",
      "36.546024322509766\n",
      "37.106632232666016\n",
      "36.984352111816406\n",
      "36.790557861328125\n",
      "37.24632263183594\n",
      "37.378440856933594\n",
      "36.50262451171875\n",
      "36.52796173095703\n",
      "37.07266616821289\n",
      "36.740665435791016\n",
      "36.85139083862305\n",
      "36.74284362792969\n",
      "36.41543960571289\n",
      "37.312259674072266\n",
      "37.51323699951172\n",
      "37.47671890258789\n",
      "36.43397903442383\n",
      "36.45197677612305\n",
      "37.50029754638672\n",
      "37.06538391113281\n",
      "37.56977081298828\n",
      "37.252254486083984\n",
      "36.486026763916016\n",
      "36.69572448730469\n",
      "36.309635162353516\n",
      "37.675926208496094\n",
      "37.7838134765625\n",
      "37.18389892578125\n",
      "36.84183120727539\n",
      "37.13211441040039\n",
      "37.0787467956543\n",
      "37.41305160522461\n",
      "36.54435348510742\n",
      "36.637210845947266\n",
      "36.8869743347168\n",
      "37.738380432128906\n",
      "37.08039474487305\n",
      "37.760746002197266\n",
      "36.844120025634766\n",
      "36.56023025512695\n",
      "36.48435592651367\n",
      "37.33649826049805\n",
      "37.62067413330078\n",
      "37.084510803222656\n",
      "36.42731857299805\n",
      "36.9802131652832\n",
      "37.008399963378906\n",
      "37.54759979248047\n",
      "37.623233795166016\n",
      "36.65638732910156\n",
      "36.62559509277344\n",
      "36.998592376708984\n",
      "37.03611755371094\n",
      "36.95994186401367\n",
      "36.95142364501953\n",
      "37.0847053527832\n",
      "37.70101547241211\n",
      "36.90658950805664\n",
      "36.443214416503906\n",
      "36.33539962768555\n",
      "36.28989028930664\n",
      "36.52627182006836\n",
      "36.4088020324707\n",
      "36.26423263549805\n",
      "37.87913131713867\n",
      "36.4990234375\n",
      "37.21201705932617\n",
      "37.12974166870117\n",
      "36.97067642211914\n",
      "36.403846740722656\n",
      "36.869300842285156\n",
      "36.30543518066406\n",
      "36.743873596191406\n",
      "36.40423583984375\n",
      "36.39726638793945\n",
      "36.40217208862305\n",
      "37.24876022338867\n",
      "37.310325622558594\n",
      "36.52471160888672\n",
      "37.269447326660156\n",
      "36.357139587402344\n",
      "36.53853988647461\n",
      "36.416351318359375\n",
      "36.29922866821289\n",
      "36.33631134033203\n",
      "36.806602478027344\n",
      "37.338016510009766\n",
      "36.330780029296875\n",
      "36.97603225708008\n",
      "36.57777404785156\n",
      "36.72583770751953\n",
      "36.43878936767578\n",
      "36.54154586791992\n",
      "36.39482116699219\n",
      "36.70612716674805\n",
      "36.91438674926758\n",
      "36.30061340332031\n",
      "36.814029693603516\n",
      "36.254825592041016\n",
      "37.51231384277344\n",
      "36.377174377441406\n",
      "36.298099517822266\n",
      "37.49968338012695\n",
      "38.24665069580078\n",
      "36.34421920776367\n",
      "37.658077239990234\n",
      "36.30311584472656\n",
      "36.59449005126953\n",
      "36.326637268066406\n",
      "37.7208137512207\n",
      "36.34902572631836\n",
      "36.737850189208984\n",
      "36.91742706298828\n",
      "36.4404411315918\n",
      "36.439910888671875\n",
      "36.40254592895508\n",
      "36.31669616699219\n",
      "36.353233337402344\n",
      "37.95960235595703\n",
      "36.940608978271484\n",
      "36.5172119140625\n",
      "36.38919448852539\n",
      "36.29765701293945\n",
      "36.38422775268555\n",
      "36.47342300415039\n",
      "36.336639404296875\n",
      "36.44831085205078\n",
      "36.40766143798828\n",
      "36.484275817871094\n",
      "36.25294494628906\n",
      "36.970664978027344\n",
      "36.394805908203125\n",
      "36.276302337646484\n",
      "36.552242279052734\n",
      "36.247825622558594\n",
      "36.75498962402344\n",
      "36.44965362548828\n",
      "36.44625473022461\n",
      "36.3898811340332\n",
      "36.385581970214844\n",
      "36.37041091918945\n",
      "36.68019485473633\n",
      "37.2963752746582\n",
      "36.26419448852539\n",
      "36.87248229980469\n",
      "36.56173324584961\n",
      "36.30492401123047\n",
      "36.357086181640625\n",
      "36.614681243896484\n",
      "37.13434600830078\n",
      "36.44876480102539\n",
      "36.24600601196289\n",
      "36.3967170715332\n",
      "36.27564239501953\n",
      "36.41978073120117\n",
      "36.61103439331055\n",
      "36.534584045410156\n",
      "36.393741607666016\n",
      "36.33170700073242\n",
      "36.6559944152832\n",
      "36.37277603149414\n",
      "36.27169418334961\n",
      "36.26121139526367\n",
      "36.32490158081055\n",
      "36.284027099609375\n",
      "36.83266067504883\n",
      "36.80308151245117\n",
      "36.327144622802734\n",
      "36.284568786621094\n",
      "36.34916305541992\n",
      "38.111297607421875\n",
      "36.24292755126953\n",
      "37.37843322753906\n",
      "36.282928466796875\n",
      "36.250709533691406\n",
      "36.2928466796875\n",
      "36.34153366088867\n",
      "36.52146911621094\n",
      "36.23831558227539\n",
      "36.5674934387207\n",
      "36.52006149291992\n",
      "36.31763458251953\n",
      "36.40324783325195\n",
      "38.08958053588867\n",
      "36.561641693115234\n",
      "36.292999267578125\n",
      "36.49953842163086\n",
      "36.33200454711914\n",
      "37.86857604980469\n",
      "36.43641662597656\n",
      "36.2552604675293\n",
      "36.38058853149414\n",
      "37.404422760009766\n",
      "36.2827262878418\n",
      "36.27230453491211\n",
      "36.29444122314453\n",
      "36.24058532714844\n",
      "37.0650749206543\n",
      "36.27286148071289\n",
      "36.33143997192383\n",
      "36.367008209228516\n",
      "36.271812438964844\n",
      "36.23496627807617\n",
      "36.334903717041016\n",
      "36.29422378540039\n",
      "36.30309295654297\n",
      "36.438720703125\n",
      "36.5390625\n",
      "37.934852600097656\n",
      "36.37638473510742\n",
      "36.25604248046875\n",
      "36.29379653930664\n",
      "36.26289749145508\n",
      "36.262855529785156\n",
      "37.26466751098633\n",
      "36.26601028442383\n",
      "36.615203857421875\n",
      "37.17865753173828\n",
      "36.36605453491211\n",
      "36.302547454833984\n",
      "37.0114631652832\n",
      "36.28424072265625\n",
      "36.40205001831055\n",
      "36.46519088745117\n",
      "36.405296325683594\n",
      "36.25065612792969\n",
      "36.27423095703125\n",
      "37.544734954833984\n",
      "36.26332092285156\n",
      "36.26139450073242\n",
      "36.246036529541016\n",
      "36.23851776123047\n",
      "36.25532913208008\n",
      "36.30766296386719\n",
      "36.55725860595703\n",
      "36.375091552734375\n",
      "36.2366943359375\n",
      "36.27582550048828\n",
      "36.302581787109375\n",
      "36.28767013549805\n",
      "36.65985870361328\n",
      "36.28670883178711\n",
      "36.39842987060547\n",
      "36.48887252807617\n",
      "38.1186408996582\n",
      "36.252471923828125\n",
      "36.33992004394531\n",
      "36.25742721557617\n",
      "36.25350570678711\n",
      "36.45603561401367\n",
      "36.2888298034668\n",
      "36.49076843261719\n",
      "36.261207580566406\n",
      "36.74927520751953\n",
      "36.30260467529297\n",
      "36.274681091308594\n",
      "36.26681900024414\n",
      "36.746498107910156\n",
      "36.42649459838867\n",
      "37.73786163330078\n",
      "36.75126647949219\n",
      "36.255088806152344\n",
      "36.7325553894043\n",
      "36.249359130859375\n",
      "36.26172637939453\n",
      "36.3185920715332\n",
      "36.327911376953125\n",
      "36.31490707397461\n",
      "36.297061920166016\n",
      "36.24745559692383\n",
      "36.363128662109375\n",
      "36.33928298950195\n",
      "36.247886657714844\n",
      "36.50811004638672\n",
      "36.35586929321289\n",
      "36.2996940612793\n",
      "36.37275314331055\n",
      "36.264892578125\n",
      "36.31167984008789\n",
      "36.318241119384766\n",
      "36.254058837890625\n",
      "36.29875183105469\n",
      "36.28927993774414\n",
      "36.347206115722656\n",
      "36.87672424316406\n",
      "36.32959747314453\n",
      "36.64767837524414\n",
      "36.34021759033203\n",
      "36.350765228271484\n",
      "36.89155578613281\n",
      "36.28565216064453\n",
      "36.51681137084961\n",
      "36.267696380615234\n",
      "36.24579620361328\n",
      "36.31173324584961\n",
      "36.299530029296875\n",
      "36.32344055175781\n",
      "36.42728042602539\n",
      "36.342403411865234\n",
      "36.482643127441406\n",
      "36.248172760009766\n",
      "36.35490417480469\n",
      "36.27967834472656\n",
      "36.28664016723633\n",
      "36.460609436035156\n",
      "36.34359359741211\n",
      "36.269256591796875\n",
      "36.39259338378906\n",
      "36.2652587890625\n",
      "36.25545120239258\n",
      "36.790138244628906\n",
      "36.304840087890625\n",
      "36.31296157836914\n",
      "36.28485107421875\n",
      "37.923988342285156\n",
      "36.24030303955078\n",
      "36.2755012512207\n",
      "36.24226760864258\n",
      "36.247066497802734\n",
      "36.240875244140625\n",
      "36.27906799316406\n",
      "36.31512451171875\n",
      "36.81562042236328\n",
      "36.50912857055664\n",
      "38.133480072021484\n",
      "36.28608322143555\n",
      "36.545127868652344\n",
      "36.245819091796875\n",
      "36.260169982910156\n",
      "36.297054290771484\n",
      "36.2586669921875\n",
      "36.92292022705078\n",
      "36.42601776123047\n",
      "36.324337005615234\n",
      "37.18729782104492\n",
      "36.43707275390625\n",
      "36.49209976196289\n",
      "37.969905853271484\n",
      "36.797054290771484\n",
      "36.281944274902344\n",
      "36.368614196777344\n",
      "36.4017219543457\n",
      "36.247745513916016\n",
      "36.320831298828125\n",
      "36.73320770263672\n",
      "36.27902603149414\n",
      "36.286617279052734\n",
      "36.400840759277344\n",
      "36.26325988769531\n",
      "36.31403350830078\n",
      "36.245079040527344\n",
      "36.545379638671875\n",
      "36.50471878051758\n",
      "36.25397491455078\n",
      "36.26099395751953\n",
      "36.24435043334961\n",
      "36.38254165649414\n",
      "36.25074768066406\n",
      "36.27348709106445\n",
      "36.31333541870117\n",
      "36.25422668457031\n",
      "36.43179702758789\n",
      "36.26913070678711\n",
      "36.400611877441406\n",
      "38.11943054199219\n",
      "38.158206939697266\n",
      "36.446502685546875\n",
      "36.23310089111328\n",
      "36.328468322753906\n",
      "36.31476593017578\n",
      "36.35040283203125\n",
      "36.49957275390625\n",
      "36.7871208190918\n",
      "36.276092529296875\n",
      "36.25225067138672\n",
      "36.35103225708008\n",
      "36.66644287109375\n",
      "36.790184020996094\n",
      "36.267608642578125\n",
      "36.3764762878418\n",
      "36.29015350341797\n",
      "36.28404998779297\n",
      "36.26117706298828\n",
      "36.370277404785156\n",
      "36.243743896484375\n",
      "36.314613342285156\n",
      "36.41978073120117\n",
      "36.72322082519531\n",
      "36.352210998535156\n",
      "36.664405822753906\n",
      "36.31884002685547\n",
      "36.34342956542969\n",
      "36.287113189697266\n",
      "36.259918212890625\n",
      "36.23987579345703\n",
      "36.32773208618164\n",
      "36.5408821105957\n",
      "37.10397720336914\n",
      "36.3317756652832\n",
      "36.30113220214844\n",
      "36.28630065917969\n",
      "36.33036804199219\n",
      "36.630863189697266\n",
      "42.764225006103516\n",
      "38.48007583618164\n",
      "36.5620002746582\n",
      "36.40365982055664\n",
      "36.62595748901367\n",
      "36.561439514160156\n",
      "36.4406852722168\n",
      "38.3964958190918\n",
      "36.77718734741211\n",
      "36.75517654418945\n",
      "37.126766204833984\n",
      "39.724342346191406\n",
      "36.7198486328125\n",
      "36.5432243347168\n",
      "37.926902770996094\n",
      "38.183658599853516\n",
      "37.553646087646484\n",
      "36.45048904418945\n",
      "36.3622932434082\n",
      "36.9220085144043\n",
      "36.31571578979492\n",
      "36.55229187011719\n",
      "36.300010681152344\n",
      "36.25739669799805\n",
      "37.7339973449707\n",
      "36.482810974121094\n",
      "36.2886962890625\n",
      "36.41645050048828\n",
      "36.432098388671875\n",
      "36.51619338989258\n",
      "38.23670959472656\n",
      "36.63087844848633\n",
      "36.43479537963867\n",
      "36.32249069213867\n",
      "36.30537414550781\n",
      "36.4857292175293\n",
      "36.314178466796875\n",
      "36.24634552001953\n",
      "36.27827072143555\n",
      "36.34022903442383\n",
      "36.42475128173828\n",
      "36.41526794433594\n",
      "38.398616790771484\n",
      "36.2503547668457\n",
      "36.36198425292969\n",
      "36.66727066040039\n",
      "36.51094055175781\n",
      "36.238460540771484\n",
      "36.351131439208984\n",
      "36.28340530395508\n",
      "36.24909591674805\n",
      "36.59916305541992\n",
      "36.42005920410156\n",
      "36.290626525878906\n",
      "36.28758239746094\n",
      "36.25878143310547\n",
      "36.266998291015625\n",
      "37.907981872558594\n",
      "38.37424850463867\n",
      "36.44948959350586\n",
      "36.265926361083984\n",
      "36.42665100097656\n",
      "37.2176513671875\n",
      "36.30685043334961\n",
      "36.42880630493164\n",
      "36.41331481933594\n",
      "36.46282958984375\n",
      "36.324623107910156\n",
      "36.60205078125\n",
      "36.51286315917969\n",
      "36.498355865478516\n",
      "36.256954193115234\n",
      "36.547550201416016\n",
      "36.27992630004883\n",
      "36.2535400390625\n",
      "36.332698822021484\n",
      "36.2968864440918\n",
      "37.13226318359375\n",
      "36.524410247802734\n",
      "36.31674575805664\n",
      "36.2810173034668\n",
      "36.263755798339844\n",
      "36.44367980957031\n",
      "36.50807571411133\n",
      "36.25577163696289\n",
      "36.2643928527832\n",
      "36.3358154296875\n",
      "36.29921340942383\n",
      "36.41128158569336\n",
      "36.252105712890625\n",
      "36.25106430053711\n",
      "36.2769889831543\n",
      "36.28844451904297\n",
      "36.30405044555664\n",
      "38.01445388793945\n",
      "36.23447799682617\n",
      "36.25473403930664\n",
      "36.346378326416016\n",
      "36.245487213134766\n",
      "36.23000717163086\n",
      "36.26158905029297\n",
      "36.734310150146484\n",
      "36.52458953857422\n",
      "37.62968444824219\n",
      "36.2880859375\n",
      "36.38273620605469\n",
      "36.839717864990234\n",
      "36.78071594238281\n",
      "36.44398880004883\n",
      "36.349395751953125\n",
      "36.40415573120117\n",
      "36.28596878051758\n",
      "37.63880157470703\n",
      "42.56282424926758\n",
      "37.25204086303711\n",
      "41.304481506347656\n",
      "36.259361267089844\n",
      "37.422325134277344\n",
      "36.57180404663086\n",
      "36.79581069946289\n",
      "36.786781311035156\n",
      "36.839019775390625\n",
      "36.472084045410156\n",
      "36.872440338134766\n",
      "36.43451690673828\n",
      "38.8485221862793\n",
      "36.43251037597656\n",
      "37.43824768066406\n",
      "37.701629638671875\n",
      "36.384307861328125\n",
      "36.31785202026367\n",
      "37.38774108886719\n",
      "36.473323822021484\n",
      "36.358280181884766\n",
      "36.3758544921875\n",
      "36.29050064086914\n",
      "36.269859313964844\n",
      "38.16447830200195\n",
      "36.43852615356445\n",
      "38.23807907104492\n",
      "37.249183654785156\n",
      "38.1455078125\n",
      "37.805904388427734\n",
      "38.357852935791016\n",
      "37.28426742553711\n",
      "38.31496810913086\n",
      "38.581974029541016\n",
      "37.420074462890625\n",
      "37.64121627807617\n",
      "37.57365417480469\n",
      "37.96114730834961\n",
      "37.278202056884766\n",
      "37.81936264038086\n",
      "37.1534538269043\n",
      "37.601627349853516\n",
      "37.03751754760742\n",
      "37.51346206665039\n",
      "37.684410095214844\n",
      "37.38634490966797\n",
      "37.89729690551758\n",
      "37.078041076660156\n",
      "36.66156768798828\n",
      "37.41916275024414\n",
      "36.97431945800781\n",
      "37.560508728027344\n",
      "36.270843505859375\n",
      "36.61737823486328\n",
      "37.662235260009766\n",
      "37.763492584228516\n",
      "37.56203842163086\n",
      "37.28308868408203\n",
      "36.4137077331543\n",
      "36.69906997680664\n",
      "36.437583923339844\n",
      "38.09929656982422\n",
      "36.35287857055664\n",
      "36.31453323364258\n",
      "36.51630783081055\n",
      "36.279911041259766\n",
      "36.43385314941406\n",
      "36.50590896606445\n",
      "36.34416198730469\n",
      "37.57773208618164\n",
      "36.3787841796875\n",
      "37.12567138671875\n",
      "36.558990478515625\n",
      "37.43838882446289\n",
      "36.93009567260742\n",
      "36.49517059326172\n",
      "36.60584259033203\n",
      "37.9010009765625\n",
      "36.25347137451172\n",
      "36.24760055541992\n",
      "36.46958923339844\n",
      "36.25342559814453\n",
      "38.035152435302734\n",
      "37.69858169555664\n",
      "36.24314498901367\n",
      "36.36601638793945\n",
      "36.27973556518555\n",
      "36.3375358581543\n",
      "37.680076599121094\n",
      "36.357303619384766\n",
      "36.45259094238281\n",
      "36.370540618896484\n",
      "36.2602653503418\n",
      "36.870731353759766\n",
      "36.641998291015625\n",
      "36.42729187011719\n",
      "36.31781005859375\n",
      "36.27758026123047\n",
      "36.68632888793945\n",
      "36.6751823425293\n",
      "36.34425735473633\n",
      "36.237945556640625\n",
      "37.59321212768555\n",
      "36.33212661743164\n",
      "36.26333999633789\n",
      "36.80091857910156\n",
      "36.259979248046875\n",
      "37.55223083496094\n",
      "36.25666427612305\n",
      "36.27115249633789\n",
      "37.865020751953125\n",
      "38.06317138671875\n",
      "36.362796783447266\n",
      "36.29991149902344\n",
      "36.51411056518555\n",
      "36.24235916137695\n",
      "36.53293991088867\n",
      "36.38556671142578\n",
      "36.4692268371582\n",
      "36.398719787597656\n",
      "36.341915130615234\n",
      "36.270423889160156\n",
      "36.32048416137695\n",
      "36.2568359375\n",
      "36.590389251708984\n",
      "36.630104064941406\n",
      "36.24989700317383\n",
      "37.804019927978516\n",
      "36.92177963256836\n",
      "36.36076354980469\n",
      "36.36833572387695\n",
      "36.300148010253906\n",
      "36.29134750366211\n",
      "38.57750701904297\n",
      "36.27665328979492\n",
      "36.2799186706543\n",
      "36.258392333984375\n",
      "36.24835968017578\n",
      "36.24492263793945\n",
      "36.231712341308594\n",
      "36.33258819580078\n",
      "36.22856521606445\n",
      "36.47887420654297\n",
      "36.45537185668945\n",
      "37.80378723144531\n",
      "36.40332794189453\n",
      "36.6329460144043\n",
      "36.35417556762695\n",
      "36.262630462646484\n",
      "36.281883239746094\n",
      "36.242122650146484\n",
      "36.23154830932617\n",
      "36.23991775512695\n",
      "36.26258850097656\n",
      "36.26502990722656\n",
      "37.972896575927734\n",
      "36.311283111572266\n",
      "36.36518859863281\n",
      "36.70219039916992\n",
      "36.52363967895508\n",
      "36.30515670776367\n",
      "36.24618911743164\n",
      "36.25362777709961\n",
      "36.28125762939453\n",
      "36.256187438964844\n",
      "36.262725830078125\n",
      "36.27210235595703\n",
      "36.25205612182617\n",
      "36.23798751831055\n",
      "36.323490142822266\n",
      "36.270790100097656\n",
      "36.25535583496094\n",
      "36.27796173095703\n",
      "36.553428649902344\n",
      "37.1563720703125\n",
      "36.257972717285156\n",
      "36.23776626586914\n",
      "38.17873001098633\n",
      "36.254478454589844\n",
      "38.01017379760742\n",
      "37.769248962402344\n",
      "36.35321807861328\n",
      "36.341705322265625\n",
      "36.30245590209961\n",
      "36.32131576538086\n",
      "36.35551452636719\n",
      "36.433998107910156\n",
      "36.28950500488281\n",
      "36.23275375366211\n",
      "36.36222839355469\n",
      "36.26640701293945\n",
      "37.873653411865234\n",
      "37.82343292236328\n",
      "36.29594802856445\n",
      "36.239925384521484\n",
      "36.272953033447266\n",
      "36.49959945678711\n",
      "37.64269256591797\n",
      "36.69716262817383\n",
      "36.33143997192383\n",
      "36.43739700317383\n",
      "36.25999450683594\n",
      "36.36137390136719\n",
      "36.34779357910156\n",
      "37.711219787597656\n",
      "36.460166931152344\n",
      "36.27360534667969\n",
      "36.24989318847656\n",
      "37.83543014526367\n",
      "36.330387115478516\n",
      "36.265159606933594\n",
      "37.658206939697266\n",
      "37.93385314941406\n",
      "36.43385314941406\n",
      "36.3688850402832\n",
      "37.77181625366211\n",
      "36.276607513427734\n",
      "36.6839714050293\n",
      "36.4054069519043\n",
      "36.2322883605957\n",
      "36.30815124511719\n",
      "36.261940002441406\n",
      "36.24358367919922\n",
      "36.24296951293945\n",
      "36.239078521728516\n",
      "36.32257080078125\n",
      "36.268218994140625\n",
      "36.284488677978516\n",
      "37.90489959716797\n",
      "38.27263641357422\n",
      "36.29684066772461\n",
      "38.25433349609375\n",
      "36.36977005004883\n",
      "36.226722717285156\n",
      "37.8419075012207\n",
      "36.329139709472656\n",
      "36.230735778808594\n",
      "36.263946533203125\n",
      "36.31422424316406\n",
      "36.30230712890625\n",
      "36.300079345703125\n",
      "36.410526275634766\n",
      "36.25033950805664\n",
      "36.48339080810547\n",
      "36.2453498840332\n",
      "36.28852081298828\n",
      "36.381229400634766\n",
      "38.109527587890625\n",
      "36.36521530151367\n",
      "36.25810623168945\n",
      "36.306396484375\n",
      "36.29008483886719\n",
      "36.23948287963867\n",
      "37.20452117919922\n",
      "36.37594985961914\n",
      "36.275699615478516\n",
      "36.232398986816406\n",
      "36.26786804199219\n",
      "36.328861236572266\n",
      "36.2913932800293\n",
      "36.262847900390625\n",
      "36.266990661621094\n",
      "36.24111557006836\n",
      "36.248748779296875\n",
      "36.30321502685547\n",
      "37.02783203125\n",
      "36.26319885253906\n",
      "36.239688873291016\n",
      "36.28989028930664\n",
      "36.255577087402344\n",
      "36.36681365966797\n",
      "36.23370361328125\n",
      "36.26921081542969\n",
      "36.34016036987305\n",
      "36.394264221191406\n",
      "36.28200149536133\n",
      "36.24258041381836\n",
      "36.23883819580078\n",
      "36.334476470947266\n",
      "36.256229400634766\n",
      "37.820892333984375\n",
      "36.39350509643555\n",
      "36.252864837646484\n",
      "36.25032043457031\n",
      "36.303504943847656\n",
      "36.2370719909668\n",
      "36.91645812988281\n",
      "36.27005386352539\n",
      "36.29030227661133\n",
      "36.2332878112793\n",
      "36.2342414855957\n",
      "36.24760055541992\n",
      "36.27055740356445\n",
      "36.24037551879883\n",
      "36.323734283447266\n",
      "36.29316329956055\n",
      "36.227378845214844\n",
      "36.28584671020508\n",
      "36.83852767944336\n",
      "36.49391174316406\n",
      "36.23768997192383\n",
      "36.2527961730957\n",
      "36.25529479980469\n",
      "36.23329544067383\n",
      "36.25194549560547\n",
      "37.94938659667969\n",
      "38.07487869262695\n",
      "36.37119674682617\n",
      "36.292789459228516\n",
      "36.262760162353516\n",
      "36.27696228027344\n",
      "36.26784896850586\n",
      "37.90748596191406\n",
      "36.267723083496094\n",
      "36.89126968383789\n",
      "36.51173782348633\n",
      "36.30653762817383\n",
      "36.28781509399414\n",
      "36.321495056152344\n",
      "36.234901428222656\n",
      "36.528076171875\n",
      "36.32420349121094\n",
      "36.39242172241211\n",
      "37.051265716552734\n",
      "36.29515838623047\n",
      "38.0302619934082\n",
      "36.46392822265625\n",
      "36.2999267578125\n",
      "36.24791717529297\n",
      "36.36634826660156\n",
      "36.27825164794922\n",
      "36.24433135986328\n",
      "36.24372482299805\n",
      "36.2664794921875\n",
      "36.41712188720703\n",
      "36.238033294677734\n",
      "36.44517517089844\n",
      "36.32135772705078\n",
      "37.96842575073242\n",
      "36.39339065551758\n",
      "36.35193634033203\n",
      "36.255638122558594\n",
      "36.2277717590332\n",
      "36.314491271972656\n",
      "38.00720977783203\n",
      "36.556636810302734\n",
      "36.25132751464844\n",
      "36.26059341430664\n",
      "37.82313919067383\n",
      "36.25785446166992\n",
      "37.85913848876953\n",
      "36.480445861816406\n",
      "36.32542419433594\n",
      "36.303741455078125\n",
      "36.28341293334961\n",
      "36.25938034057617\n",
      "36.23358917236328\n",
      "36.266014099121094\n",
      "36.247955322265625\n",
      "36.93169403076172\n",
      "37.81460189819336\n",
      "36.29378128051758\n",
      "36.262611389160156\n",
      "36.304931640625\n",
      "36.255615234375\n",
      "36.33557891845703\n",
      "36.27510452270508\n",
      "36.263694763183594\n",
      "37.834686279296875\n",
      "36.31161880493164\n",
      "36.33393096923828\n",
      "36.27527618408203\n",
      "36.255043029785156\n",
      "36.23769760131836\n",
      "36.25940704345703\n",
      "36.23853302001953\n",
      "36.24529266357422\n",
      "36.30595397949219\n",
      "36.23329544067383\n",
      "36.30059814453125\n",
      "36.24442672729492\n",
      "36.343021392822266\n",
      "36.23383712768555\n",
      "36.23940658569336\n",
      "36.23255157470703\n",
      "37.852481842041016\n",
      "36.25724792480469\n",
      "36.24036407470703\n",
      "36.29962921142578\n",
      "36.23551559448242\n",
      "36.28966522216797\n",
      "36.25177764892578\n",
      "36.234657287597656\n",
      "36.23727798461914\n",
      "36.29397201538086\n",
      "36.239994049072266\n",
      "36.30060577392578\n",
      "36.238677978515625\n",
      "36.26413345336914\n",
      "38.01140213012695\n",
      "36.277732849121094\n",
      "36.325950622558594\n",
      "36.254634857177734\n",
      "36.24320602416992\n",
      "36.26277160644531\n",
      "36.27122116088867\n",
      "36.3797492980957\n",
      "36.26779556274414\n",
      "36.24919891357422\n",
      "36.24406814575195\n",
      "37.71662521362305\n",
      "36.30258560180664\n",
      "36.26715087890625\n",
      "37.913299560546875\n",
      "37.87451171875\n",
      "36.429195404052734\n",
      "37.72037124633789\n",
      "36.30554962158203\n",
      "36.35101318359375\n",
      "36.75347900390625\n",
      "36.23249435424805\n",
      "36.443355560302734\n",
      "36.2791633605957\n",
      "36.81338119506836\n",
      "36.26620864868164\n",
      "36.30538558959961\n",
      "36.25497817993164\n",
      "36.28068923950195\n",
      "36.247379302978516\n",
      "38.025543212890625\n",
      "36.24074172973633\n",
      "36.271629333496094\n",
      "36.2494010925293\n",
      "36.335716247558594\n",
      "36.67371368408203\n",
      "36.391361236572266\n",
      "36.23817825317383\n",
      "36.26731872558594\n",
      "36.245452880859375\n",
      "36.340484619140625\n",
      "36.271400451660156\n",
      "36.373008728027344\n",
      "36.27080154418945\n",
      "37.800594329833984\n",
      "36.366146087646484\n",
      "36.32799530029297\n",
      "36.2418212890625\n",
      "36.24187469482422\n",
      "36.273963928222656\n",
      "36.26289367675781\n",
      "36.2902946472168\n",
      "36.270957946777344\n",
      "36.22835159301758\n",
      "36.24484634399414\n",
      "36.285274505615234\n",
      "36.243003845214844\n",
      "38.04334259033203\n",
      "36.29349136352539\n",
      "36.285186767578125\n",
      "36.26661682128906\n",
      "36.267555236816406\n",
      "36.290077209472656\n",
      "36.496463775634766\n",
      "36.240447998046875\n",
      "36.25313186645508\n",
      "36.45687484741211\n",
      "36.3181266784668\n",
      "36.27616882324219\n",
      "36.348262786865234\n",
      "36.279212951660156\n",
      "36.26911926269531\n",
      "36.305728912353516\n",
      "36.289024353027344\n",
      "36.289058685302734\n",
      "36.24729919433594\n",
      "36.27006530761719\n",
      "36.247398376464844\n",
      "36.34773635864258\n",
      "36.25638198852539\n",
      "36.26557159423828\n",
      "36.25336456298828\n",
      "36.27600860595703\n",
      "36.30499267578125\n",
      "36.31437683105469\n",
      "36.2658805847168\n",
      "36.304481506347656\n",
      "36.24253845214844\n",
      "36.261451721191406\n",
      "36.2778205871582\n",
      "36.37281036376953\n",
      "36.39064407348633\n",
      "36.32188415527344\n",
      "37.82847213745117\n",
      "36.286293029785156\n",
      "36.23447799682617\n",
      "36.255741119384766\n",
      "36.347023010253906\n",
      "36.32534408569336\n",
      "36.37088394165039\n",
      "36.2540283203125\n",
      "37.85245895385742\n",
      "36.28318405151367\n",
      "36.2943000793457\n",
      "36.31562423706055\n",
      "36.35820770263672\n",
      "36.50436782836914\n",
      "36.3758430480957\n",
      "36.32233428955078\n",
      "36.24742889404297\n",
      "36.319602966308594\n",
      "36.30306625366211\n",
      "36.25791549682617\n",
      "36.32477951049805\n",
      "36.28345489501953\n",
      "37.844757080078125\n",
      "36.26313018798828\n",
      "36.24263381958008\n",
      "36.24924087524414\n",
      "36.4466438293457\n",
      "36.278106689453125\n",
      "36.284446716308594\n",
      "36.275367736816406\n",
      "36.271663665771484\n",
      "36.348793029785156\n",
      "37.835792541503906\n",
      "36.27922058105469\n",
      "36.310813903808594\n",
      "36.32603073120117\n",
      "36.27396011352539\n",
      "37.952884674072266\n",
      "36.29869079589844\n",
      "36.27015686035156\n",
      "36.40283966064453\n",
      "37.95246124267578\n",
      "36.31767272949219\n",
      "36.30659866333008\n",
      "36.679588317871094\n",
      "36.364192962646484\n",
      "36.39512252807617\n",
      "36.26819610595703\n",
      "36.27016830444336\n",
      "36.3248176574707\n",
      "36.290008544921875\n",
      "36.37553024291992\n",
      "36.29885482788086\n",
      "36.34009552001953\n",
      "37.76280212402344\n",
      "36.255157470703125\n",
      "36.50405502319336\n",
      "36.67959976196289\n",
      "36.2811164855957\n",
      "36.34675979614258\n",
      "36.4089241027832\n",
      "37.94012451171875\n",
      "36.26088333129883\n",
      "36.321990966796875\n",
      "37.86578369140625\n",
      "36.53114700317383\n",
      "37.93174743652344\n",
      "36.443016052246094\n",
      "36.52686309814453\n",
      "36.59070587158203\n",
      "36.47628402709961\n",
      "36.29658508300781\n",
      "36.52663040161133\n",
      "36.38771057128906\n",
      "38.11699295043945\n",
      "36.63067626953125\n",
      "36.316402435302734\n",
      "36.26824188232422\n",
      "36.32062911987305\n",
      "36.287437438964844\n",
      "36.40548324584961\n",
      "37.160682678222656\n",
      "37.77307891845703\n",
      "36.262840270996094\n",
      "36.31742858886719\n",
      "36.328338623046875\n",
      "36.45398712158203\n",
      "36.2991943359375\n",
      "38.47970962524414\n",
      "36.26467514038086\n",
      "36.29668426513672\n",
      "36.34162902832031\n",
      "36.37645721435547\n",
      "36.391387939453125\n",
      "36.37835693359375\n",
      "36.326927185058594\n",
      "36.39973449707031\n",
      "36.38069152832031\n",
      "36.33665084838867\n",
      "36.39934158325195\n",
      "36.35819625854492\n",
      "36.464027404785156\n",
      "36.49207305908203\n",
      "36.33151626586914\n",
      "36.27928161621094\n",
      "36.443763732910156\n",
      "36.30857467651367\n",
      "36.27894973754883\n",
      "36.338958740234375\n",
      "36.32944869995117\n",
      "36.28307342529297\n",
      "36.362998962402344\n",
      "36.38616943359375\n",
      "36.25389862060547\n",
      "36.36520767211914\n",
      "36.28717803955078\n",
      "36.363555908203125\n",
      "36.4050178527832\n",
      "36.4209098815918\n",
      "36.40812683105469\n",
      "36.32106018066406\n",
      "36.24140167236328\n",
      "38.13681411743164\n",
      "38.80918502807617\n",
      "36.314842224121094\n",
      "36.52238845825195\n",
      "36.45164108276367\n",
      "36.364715576171875\n",
      "36.399383544921875\n",
      "38.01924133300781\n",
      "37.82087707519531\n",
      "36.42237854003906\n",
      "36.388671875\n",
      "36.42442321777344\n",
      "37.84296417236328\n",
      "36.35184097290039\n",
      "36.48857116699219\n",
      "36.3283576965332\n",
      "38.17994689941406\n",
      "36.552886962890625\n",
      "36.30439758300781\n",
      "36.454307556152344\n",
      "36.48842239379883\n",
      "36.36980056762695\n",
      "37.87456130981445\n",
      "36.435707092285156\n",
      "37.866294860839844\n",
      "36.40074157714844\n",
      "36.599910736083984\n",
      "36.36333465576172\n",
      "36.51694869995117\n",
      "36.39796447753906\n",
      "36.59109878540039\n",
      "36.95078659057617\n",
      "36.40935134887695\n",
      "36.32491683959961\n",
      "36.46929931640625\n",
      "36.35248565673828\n",
      "36.340763092041016\n",
      "38.03426742553711\n",
      "36.36858367919922\n",
      "36.36767578125\n",
      "36.63068389892578\n",
      "36.491485595703125\n",
      "36.456809997558594\n",
      "36.55983352661133\n",
      "36.32130813598633\n",
      "36.36432647705078\n",
      "36.642417907714844\n",
      "36.416648864746094\n",
      "36.33382034301758\n",
      "36.583683013916016\n",
      "36.278663635253906\n",
      "38.01335906982422\n",
      "36.4698486328125\n",
      "36.33833694458008\n",
      "36.5026741027832\n",
      "36.26411819458008\n",
      "36.381591796875\n",
      "36.32952117919922\n",
      "36.30686569213867\n",
      "38.08704376220703\n",
      "38.00362777709961\n",
      "36.34737014770508\n",
      "36.25851058959961\n",
      "36.30657958984375\n",
      "36.29579544067383\n",
      "36.40589141845703\n",
      "38.082645416259766\n",
      "36.35383605957031\n",
      "37.84750747680664\n",
      "36.4078254699707\n",
      "36.30139923095703\n",
      "36.4844856262207\n",
      "36.381690979003906\n",
      "36.38270568847656\n",
      "36.38847732543945\n",
      "36.3377799987793\n",
      "36.306087493896484\n",
      "37.924964904785156\n",
      "36.327880859375\n",
      "36.29328918457031\n",
      "36.519752502441406\n",
      "36.511329650878906\n",
      "37.72224426269531\n",
      "37.99610137939453\n",
      "36.3140869140625\n",
      "36.34178924560547\n",
      "36.28950500488281\n",
      "37.699684143066406\n",
      "36.32963562011719\n",
      "36.41689682006836\n",
      "36.46209716796875\n",
      "36.70150375366211\n",
      "36.3408203125\n",
      "36.29697799682617\n",
      "36.37995910644531\n",
      "36.62262725830078\n",
      "36.44683074951172\n",
      "36.26725769042969\n",
      "36.292388916015625\n",
      "36.398406982421875\n",
      "36.27517318725586\n",
      "36.28337097167969\n",
      "36.597782135009766\n",
      "36.31010818481445\n",
      "36.35653305053711\n",
      "36.31200408935547\n",
      "38.115753173828125\n",
      "36.32768249511719\n",
      "36.28922653198242\n",
      "36.25286102294922\n",
      "36.276344299316406\n",
      "37.93239974975586\n",
      "36.34199523925781\n",
      "36.38259506225586\n",
      "36.3551025390625\n",
      "36.28242492675781\n",
      "38.00188446044922\n",
      "36.29313278198242\n",
      "36.41390609741211\n",
      "36.424156188964844\n",
      "36.32554244995117\n",
      "36.39426040649414\n",
      "36.36840057373047\n",
      "36.30519104003906\n",
      "36.35007095336914\n",
      "36.4975700378418\n",
      "37.73728561401367\n",
      "36.37919998168945\n",
      "36.24094772338867\n",
      "36.34311294555664\n",
      "36.442604064941406\n",
      "36.33240509033203\n",
      "36.45310974121094\n",
      "36.37385940551758\n",
      "36.32794189453125\n",
      "36.36630630493164\n",
      "38.035682678222656\n",
      "36.373069763183594\n",
      "38.214324951171875\n",
      "36.41620635986328\n",
      "36.27187728881836\n",
      "37.8377571105957\n",
      "37.88142395019531\n",
      "37.782127380371094\n",
      "36.35469436645508\n",
      "36.44221496582031\n",
      "37.8469352722168\n",
      "36.35773849487305\n",
      "36.35115051269531\n",
      "36.329769134521484\n",
      "36.354270935058594\n",
      "36.4367790222168\n",
      "36.40053939819336\n",
      "36.38218688964844\n",
      "36.30093765258789\n",
      "36.30720138549805\n",
      "36.28055953979492\n",
      "36.3842887878418\n",
      "36.396446228027344\n",
      "36.297760009765625\n",
      "36.41762161254883\n",
      "36.38654327392578\n",
      "36.34098815917969\n",
      "36.3173828125\n",
      "36.421688079833984\n",
      "36.28546905517578\n",
      "38.11642074584961\n",
      "36.4713020324707\n",
      "36.348228454589844\n",
      "36.261749267578125\n",
      "36.29484939575195\n",
      "36.32179260253906\n",
      "36.4144401550293\n",
      "36.312129974365234\n",
      "38.172760009765625\n",
      "36.2653694152832\n",
      "38.27702331542969\n",
      "36.329261779785156\n",
      "36.59299087524414\n",
      "36.45151138305664\n",
      "38.0217399597168\n",
      "36.30189895629883\n",
      "37.77410125732422\n",
      "37.7073974609375\n",
      "36.36457824707031\n",
      "36.344635009765625\n",
      "36.5020637512207\n",
      "36.536617279052734\n",
      "37.70735168457031\n",
      "36.46006774902344\n",
      "36.397823333740234\n",
      "36.497493743896484\n",
      "36.4245491027832\n",
      "36.40925979614258\n",
      "36.52589416503906\n",
      "36.36007308959961\n",
      "36.418983459472656\n",
      "36.27198791503906\n",
      "36.26850891113281\n",
      "36.28687286376953\n",
      "36.277305603027344\n",
      "36.27718734741211\n",
      "36.368186950683594\n",
      "36.281463623046875\n",
      "36.3164176940918\n",
      "36.25851058959961\n",
      "36.29016876220703\n",
      "36.32958221435547\n",
      "36.38197708129883\n",
      "36.40450668334961\n",
      "36.32194519042969\n",
      "36.29833984375\n",
      "36.27799987792969\n",
      "36.294185638427734\n",
      "38.2327880859375\n",
      "38.08096694946289\n",
      "36.36223602294922\n",
      "36.42391586303711\n",
      "36.25074768066406\n",
      "36.38331985473633\n",
      "36.257102966308594\n",
      "36.26370620727539\n",
      "36.26359558105469\n",
      "37.894798278808594\n",
      "36.2733039855957\n",
      "36.3874626159668\n",
      "36.29922103881836\n",
      "36.27397537231445\n",
      "36.278194427490234\n",
      "37.71714782714844\n",
      "36.27533721923828\n",
      "36.461483001708984\n",
      "36.45047378540039\n",
      "36.333805084228516\n",
      "36.34910202026367\n",
      "36.35073471069336\n",
      "37.7161865234375\n",
      "36.25457763671875\n",
      "36.35634231567383\n",
      "36.31304931640625\n",
      "36.275020599365234\n",
      "36.27427291870117\n",
      "38.166595458984375\n",
      "36.326541900634766\n",
      "36.29460906982422\n",
      "36.676780700683594\n",
      "36.233123779296875\n",
      "37.909690856933594\n",
      "37.78526306152344\n",
      "36.29782485961914\n",
      "36.29016876220703\n",
      "36.28443145751953\n",
      "37.77911376953125\n",
      "36.29708480834961\n",
      "36.541770935058594\n",
      "36.30891418457031\n",
      "36.264495849609375\n",
      "36.39019012451172\n",
      "36.404815673828125\n",
      "36.40753173828125\n",
      "36.28450393676758\n",
      "36.287628173828125\n",
      "36.2779426574707\n",
      "36.315006256103516\n",
      "36.264862060546875\n",
      "36.401588439941406\n",
      "36.30315017700195\n",
      "36.28135681152344\n",
      "36.31447219848633\n",
      "36.25383758544922\n",
      "38.23041534423828\n",
      "36.283973693847656\n",
      "36.324668884277344\n",
      "38.14029312133789\n",
      "36.265533447265625\n",
      "36.27236557006836\n",
      "36.25508117675781\n",
      "36.3618049621582\n",
      "38.043495178222656\n",
      "36.43461990356445\n",
      "36.25346755981445\n",
      "36.266475677490234\n",
      "36.52690124511719\n",
      "36.2664794921875\n",
      "36.44921875\n",
      "36.2916145324707\n",
      "36.31132125854492\n",
      "36.29550552368164\n",
      "36.41907501220703\n",
      "36.35295486450195\n",
      "36.340049743652344\n",
      "36.28948974609375\n",
      "36.33782196044922\n",
      "36.25743103027344\n",
      "36.272335052490234\n",
      "37.936981201171875\n",
      "36.55928421020508\n",
      "38.20323944091797\n",
      "36.37775421142578\n",
      "36.389854431152344\n",
      "36.360694885253906\n",
      "36.29639434814453\n",
      "36.455711364746094\n",
      "36.29689025878906\n",
      "37.75225067138672\n",
      "36.31698226928711\n",
      "36.34052276611328\n",
      "37.774681091308594\n",
      "36.25410079956055\n",
      "36.35468673706055\n",
      "36.35850143432617\n",
      "36.50078582763672\n",
      "36.32969284057617\n",
      "36.31584167480469\n",
      "36.335540771484375\n",
      "36.39518356323242\n",
      "36.355003356933594\n",
      "36.32060241699219\n",
      "36.3466796875\n",
      "36.47181701660156\n",
      "36.31345748901367\n",
      "36.24592208862305\n",
      "36.23700714111328\n",
      "36.270660400390625\n",
      "36.33659744262695\n",
      "36.29935836791992\n",
      "36.348426818847656\n",
      "36.250247955322266\n",
      "36.24776077270508\n",
      "38.20115280151367\n",
      "36.3606071472168\n",
      "36.246726989746094\n",
      "Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 256)\n",
    "        self.conv2 = GCNConv(256, 128)\n",
    "        self.conv3 = GCNConv(128, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN().to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [07:02<00:00,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_explanations = gbp_exps(model, graph, data, gbp)\n",
    "print(\"GBP Exps finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [33:47<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Suff finished\n",
      "Median Sufficiencies:\n",
      "\tGuidedBP: 0.6170863091945649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp_suff_30 = sufficiency(gbp_explanations, data, model)\n",
    "print(\"GBP Suff finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tGuidedBP: {median(gbp_suff_30)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [00:26<00:00, 123.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [34:26<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Suff Finished\n",
      "Median Sufficiencies:\n",
      "\tCAM: 0.582310865726322\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cam = CAM(model=model)\n",
    "\n",
    "\n",
    "cam_explanations = cam_exps(model, graph, data, cam)\n",
    "print(\"CAM Exps finished\")\n",
    "cam_suff_30 = sufficiency(cam_explanations, data, model)\n",
    "print(\"CAM Suff Finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tCAM: {median(cam_suff_30)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [13:02<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Exps Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [31:14<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Suff Finished\n",
      "Median Sufficiencies:\n",
      "\tIG: 0.3906620264053345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ig_explanations = ig_exps(model, graph, data)\n",
    "print(\"IG Exps Finished\")\n",
    "# getting Sufficiency for all explanations\n",
    "ig_suff_30 = sufficiency(ig_explanations, data, model)\n",
    "print(\"IG Suff Finished\")\n",
    "\n",
    "print(f\"Median Sufficiencies:\\n\\tIG: {median(ig_suff_30)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [09:42<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [36:22<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Suff finished\n",
      "Median Sufficiencies:\n",
      "\tGuidedBP: 0.6313642218708992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_explanations = gbp_exps(model, graph, data, gbp)\n",
    "print(\"GBP Exps finished\")\n",
    "gbp_suff_30 = sufficiency(gbp_explanations, data, model)\n",
    "print(\"GBP Suff finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tGuidedBP: {median(gbp_suff_30)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [00:31<00:00, 105.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [38:34<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Suff Finished\n",
      "Median Sufficiencies:\n",
      "\tCAM: 0.5885432839393616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cam = CAM(model=model)\n",
    "\n",
    "\n",
    "cam_explanations = cam_exps(model, graph, data, cam)\n",
    "print(\"CAM Exps finished\")\n",
    "cam_suff_30 = sufficiency(cam_explanations, data, model)\n",
    "print(\"CAM Suff Finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tCAM: {median(cam_suff_30)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [13:20<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Exps Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [37:11<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG Suff Finished\n",
      "Median Sufficiencies:\n",
      "\tIG: 0.3995195984840393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ig_explanations = ig_exps(model, graph, data)\n",
    "print(\"IG Exps Finished\")\n",
    "# getting Sufficiency for all explanations\n",
    "ig_suff_30 = sufficiency(ig_explanations, data, model)\n",
    "print(\"IG Suff Finished\")\n",
    "\n",
    "print(f\"Median Sufficiencies:\\n\\tIG: {median(ig_suff_30)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from statistics import median\n",
    "torch.manual_seed(12345)\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1 = GCNConv(dataset[0].num_nodes, 16)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [07:45<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [33:53<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP Suff finished\n",
      "Median Sufficiencies:\n",
      "\tGuidedBP: 0.6523317113518715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gbp = GuidedBP(model=model)\n",
    "\n",
    "# getting explanations for each model\n",
    "gbp_explanations = gbp_exps(model, graph, data, gbp)\n",
    "print(\"GBP Exps finished\")\n",
    "gbp_suff_30 = sufficiency(gbp_explanations, data, model)\n",
    "print(\"GBP Suff finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tGuidedBP: {median(gbp_suff_30)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [00:29<00:00, 113.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Exps finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3327/3327 [36:57<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM Suff Finished\n",
      "Median Sufficiencies:\n",
      "\tCAM: 0.608046293258667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cam = CAM(model=model)\n",
    "\n",
    "\n",
    "cam_explanations = cam_exps(model, graph, data, cam)\n",
    "print(\"CAM Exps finished\")\n",
    "cam_suff_30 = sufficiency(cam_explanations, data, model)\n",
    "print(\"CAM Suff Finished\")\n",
    "print(f\"Median Sufficiencies:\\n\\tCAM: {median(cam_suff_30)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 385/3327 [01:31<11:50,  4.14it/s]"
     ]
    }
   ],
   "source": [
    "ig_explanations = ig_exps(model, graph, data)\n",
    "print(\"IG Exps Finished\")\n",
    "# getting Sufficiency for all explanations\n",
    "ig_suff_30 = sufficiency(ig_explanations, data, model)\n",
    "print(\"IG Suff Finished\")\n",
    "\n",
    "print(f\"Median Sufficiencies:\\n\\tIG: {median(ig_suff_30)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [1:29:04<00:00, 53.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from statistics import median\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "edges = data.edge_index\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "losses = list()\n",
    "faithfulness_list = list()\n",
    "accuracy = list()\n",
    "model.train()\n",
    "from tqdm import tqdm\n",
    "with tqdm(total=100) as pbar:\n",
    "    for epoch in range(100):\n",
    "        pbar.update(1)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        # the step is now to approximate explanations for all nodes and get the median and then bobs your uncle\n",
    "        cam_exps = list()\n",
    "        model.eval()\n",
    "        cam = CAM(model=model)\n",
    "        for node in graph.nodes():\n",
    "            ig_explanations = cam.get_explanation_node(node_idx=node, x = data.x, edge_index = data.edge_index,  y = data.y)\n",
    "            cam_exps.append(ig_explanations)\n",
    "            \n",
    "        PGI = median(pgi(cam_exps, data, model, top_k=0.25))\n",
    "        model.train()\n",
    "        losses.append(int(loss.item()))\n",
    "        faithfulness_list.append(PGI)\n",
    "        \n",
    "        loss = loss + PGI\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "        acc = int(correct) / int(data.test_mask.sum())\n",
    "        accuracy.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12de92d72b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.plot([i for i in faithfulness_list], label=\"PGI\")\n",
    "plt.plot(accuracy, label=\"Accuracy\")\n",
    "plt.title(\"Loss, PGI & Accuracy vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12df08e2e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y=accuracy, x=faithfulness_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m cam \u001b[38;5;241m=\u001b[39m CAM(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[1;32m----> 4\u001b[0m     ig_explanations \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_explanation_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     cam_exps\u001b[38;5;241m.\u001b[39mappend(ig_explanations)\n\u001b[0;32m      7\u001b[0m PGI \u001b[38;5;241m=\u001b[39m median(pgi(cam_exps, data, model, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m))\n",
      "File \u001b[1;32md:\\coding\\graphxai\\graphxai\\explainers\\cam.py:86\u001b[0m, in \u001b[0;36mCAM.get_explanation_node\u001b[1;34m(self, x, node_idx, edge_index, label, y, forward_kwargs, directed)\u001b[0m\n\u001b[0;32m     83\u001b[0m         label \u001b[38;5;241m=\u001b[39m y[node_idx]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Perform walk:\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m walk_steps, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_fc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforward_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Get subgraph:\u001b[39;00m\n\u001b[0;32m     89\u001b[0m khop_info \u001b[38;5;241m=\u001b[39m k_hop_subgraph(node_idx \u001b[38;5;241m=\u001b[39m node_idx, num_hops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL, edge_index \u001b[38;5;241m=\u001b[39m edge_index)\n",
      "File \u001b[1;32md:\\coding\\graphxai\\graphxai\\explainers\\_decomp_base_old.py:73\u001b[0m, in \u001b[0;36m_BaseDecomposition.extract_step\u001b[1;34m(self, x, edge_index, detach, split_fc, forward_kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, edge_index)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# --------------------------------\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Remove hooks:\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1543\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    208\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m     96\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\freis\\anaconda3\\envs\\baseEnv\\lib\\site-packages\\torch_geometric\\utils\\loop.py:342\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    339\u001b[0m N \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m    340\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 342\u001b[0m loop_index \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m loop_index \u001b[38;5;241m=\u001b[39m loop_index\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cam = CAM(model=model)\n",
    "for node in graph.nodes():\n",
    "    ig_explanations = cam.get_explanation_node(node_idx=node, x = data.x, edge_index = data.edge_index,  y = data.y)\n",
    "    cam_exps.append(ig_explanations)\n",
    "    \n",
    "PGI = median(pgi(cam_exps, data, model, top_k=0.25))\n",
    "print(\"Baseline PGI is: {}\".format(PGI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "graph = to_networkx(data)\n",
    "edges = data.edge_index\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "losses = list()\n",
    "faithfulness_list = list()\n",
    "accuracy = list()\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline PGI is: 0.12451482090182253\n",
      "Accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cam = CAM(model=model)\n",
    "for node in graph.nodes():\n",
    "    ig_explanations = cam.get_explanation_node(node_idx=node, x = data.x, edge_index = data.edge_index,  y = data.y)\n",
    "    cam_exps.append(ig_explanations)\n",
    "PGI = median(pgi(cam_exps, data, model, top_k=0.25))\n",
    "print(\"Baseline PGI is: {}\".format(PGI))\n",
    "model.eval()\n",
    "pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
